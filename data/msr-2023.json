[
    {
        "title": "Understanding the Time to First Response In GitHub Pull Requests",
        "topics": "Technical Papers",
        "abstract": "The pull-based development paradigm is widely adopted by modern open-source software (OSS) projects, enabling a pull request (PR) to pass through multiple validation stages, from PR assignment and continuous integration testing to the actual code review, before eventually being merged into the project or rejected. Due to the distributed collaboration characteristics of open-source projects, PRs often get delayed across the PR stages, including for the first response, slowing down software development.",
        "authors": [
            {
                "name": "Kazi Amit Hasan",
                "institution": "Queen's University, Canada",
                "country": "Canada"
            },
            {
                "name": "Marcos Macedo",
                "institution": "Queen's University at Kingston / Universidad de Montevideo",
                "country": "Canada"
            },
            {
                "name": "Yuan Tian",
                "institution": "Queens University, Kingston, Canada",
                "country": "Canada"
            },
            {
                "name": "Bram Adams",
                "institution": "Queen's University, Kingston, Ontario",
                "country": "Canada"
            },
            {
                "name": "Ding Steven, H., H.",
                "institution": "Queen\u2019s University at Kingston",
                "country": null
            }
        ]
    },
    {
        "title": "Dealing with Popularity Bias in Recommender Systems for Third-party Libraries: How far Are We?",
        "topics": "Technical Papers",
        "abstract": "Recommender systems for software engineering (RSSEs) assist software engineers in dealing with a growing information overload when discerning alternative development solutions. While RSSEs are becoming more and more effective in suggesting handy recommendations, they tend to suffer from popularity bias, i.e., they favor items that are relevant mainly because several developers are using them. While this rewards artifacts that are likely more reliable and well-documented, it would also mean missing artifacts rarely used because they are very specific or more recent. This paper studies popularity bias in Third-Party Library (TPL) RSSEs. First, we investigate whether state-of-the-art research in RSSEs has already tackled the issue of popularity bias. Then, we quantitatively assess four existing TPL RSSEs, exploring their capability to deal with the recommendation of popular items. Finally, we propose a mechanism to defuse popularity bias in the recommendation list. The empirical study reveals that the issue of dealing with popularity in TPL recommender systems has not received adequate attention from the software engineering community. Among the surveyed work, only one starts investigating the issue, albeit getting a low prediction performance.",
        "authors": [
            {
                "name": "Phuong T. Nguyen",
                "institution": "University of L\u2019Aquila",
                "country": "Italy"
            },
            {
                "name": "Riccardo Rubei",
                "institution": "University of L'Aquila",
                "country": "Italy"
            },
            {
                "name": "Juri Di Rocco",
                "institution": "University of L'Aquila",
                "country": "Italy"
            },
            {
                "name": "Claudio Di Sipio",
                "institution": "University of L'Aquila",
                "country": "Italy"
            },
            {
                "name": "Davide Di Ruscio",
                "institution": "University of L'Aquila",
                "country": "Italy"
            },
            {
                "name": "Massimiliano Di Penta",
                "institution": "University of Sannio, Italy",
                "country": "Italy"
            }
        ]
    },
    {
        "title": "Smart Contract Upgradeability on the Ethereum Blockchain Platform: An Exploratory Study",
        "topics": "Registered Reports",
        "abstract": "Smart contracts are computerized self-executing contracts that contain clauses, which are enforced once certain conditions are met. Smart contracts are immutable by design and cannot be modified once deployed, which ensures trustlessness. Despite smart contracts\u2019 immutability benefits, upgrading contract code is still necessary for bug fixes and potential feature improvements. In the past few years, the smart contract community introduced several practices for upgrading smart contracts. Upgradeable contracts are smart contracts that exhibit these practices and are designed with upgradeability in mind. During the upgrade process, a new smart contract version is deployed with the desired modification, and subsequent user requests will be forwarded to the latest version (upgraded contract). Nevertheless, little is known about the characteristics of the upgrading practices, how developers apply them, and how upgrading impacts contract usage.",
        "authors": [
            {
                "name": "Ilham Qasse",
                "institution": "Reykjavik University",
                "country": "Iceland"
            },
            {
                "name": "Mohammad Hamdaqa",
                "institution": "Polytechnique Montr\u00e9al",
                "country": "Canada"
            },
            {
                "name": "Bj\u00f6rn \u00de\u00f3r J\u00f3nsson",
                "institution": "Reykjavik University",
                "country": null
            }
        ]
    },
    {
        "title": "An Exploratory Study of Ad Hoc Parsers in Python",
        "topics": "Registered Reports",
        "abstract": "Ad hoc parsers are pieces of code that use common string functions likesplit,trim, orsliceto effectively perform parsing. Whether it is handling command-line arguments, reading configuration files, parsing custom file formats, or any number of other minor string processing tasks, ad hoc parsing is ubiquitous\u2014yet poorly understood.",
        "authors": [
            {
                "name": "Michael Schr\u00f6der",
                "institution": "TU Wien",
                "country": "Austria"
            },
            {
                "name": "J\u00fcrgen Cito",
                "institution": "TU Wien",
                "country": "Austria"
            }
        ]
    },
    {
        "title": "Improving Agile Planning for Reliable Software Delivery",
        "topics": "Industry Track",
        "abstract": "Agile software development prioritizes the delivery of working software. However, there are challenges in the sprint planning process that could impact the reliability of sprint delivery. In this paper, we list three challenges related to the sprint planning process. We also discuss future work directions to facilitate the sprint planning process and mitigate those challenges for software teams.",
        "authors": [
            {
                "name": "Jirat Pasuksmit",
                "institution": "Atlassian",
                "country": "Australia"
            },
            {
                "name": "Fan Jiang",
                "institution": "Atlassian",
                "country": null
            },
            {
                "name": "Kemp Thornton",
                "institution": "Atlassian",
                "country": null
            },
            {
                "name": "Arik Friedman",
                "institution": "Atlassian",
                "country": null
            },
            {
                "name": "Natalija Fuksmane",
                "institution": "Atlassian",
                "country": null
            },
            {
                "name": "Isabelle Kohout",
                "institution": "Atlassian",
                "country": null
            },
            {
                "name": "Julian Connor",
                "institution": "Atlassian",
                "country": null
            }
        ]
    },
    {
        "title": "AutoML from Software Engineering Perspective: Landscapes and Challenges",
        "topics": "Technical Papers",
        "abstract": "Machine learning (ML) has been widely adopted in modern software, but the manual configuration of ML (e.g., hyper-parameter configuration) poses a significant challenge to software developers. Therefore, automated ML (AutoML), which seeks the optimal configuration of ML automatically, has received increasing attention from the software engineering community. However, to date, there is no comprehensive understanding of how AutoML is used by developers and what challenges developers encounter in using AutoML for software development. To fill this knowledge gap, we conduct the first study on understanding the use and challenges of AutoML from software developers\u2019 perspective. We collect and analyze 1,554 AutoML downstream repositories, 769 AutoML-related Stack Overflow questions, and 1,437 relevant GitHub issues. The results suggest the increasing popularity of AutoML in a wide range of topics, but also the lack of relevant expertise. We manually identify specific challenges faced by developers for AutoML-enabled software. Based on the results, we derive a series of implications for AutoML framework selection, framework development, and research. Code scripts and datasets are publicly available.",
        "authors": [
            {
                "name": "Chao Wang",
                "institution": "Peking University",
                "country": "China"
            },
            {
                "name": "Zhenpeng Chen",
                "institution": "University College London, UK",
                "country": "United Kingdom"
            },
            {
                "name": "Minghui Zhou",
                "institution": "Peking University",
                "country": "China"
            }
        ]
    },
    {
        "title": "Characterizing and Understanding Software Security Vulnerabilities in Machine Learning Libraries",
        "topics": "Technical Papers",
        "abstract": "The application of machine learning (ML) libraries has tremendously increased in many domains, including autonomous driving systems, medical, and critical industries. Vulnerabilities of such libraries could result in irreparable consequences. However, the characteristics of software security vulnerabilities have not been well studied. In this paper, to bridge this gap, we take the first step towards characterizing and understanding the security vulnerabilities of seven well- known ML libraries, including TensorFlow, PyTorch, Scikit-learn, Mlpack, Pandas, Numpy, and Scipy. To do so, we collected 683 security vulnerabilities to explore four major factors: 1) vulnerability types, 2) root causes, 3) symptoms, and 4) fixing patterns of security vulnerabilities in ML libraries. The findings of this study can help developers and researchers understand the characteristics of security vulnerabilities across different ML libraries.",
        "authors": [
            {
                "name": "Nima Shiri Harzevili",
                "institution": "York University",
                "country": "Canada"
            },
            {
                "name": "Jiho Shin",
                "institution": "York University",
                "country": "Canada"
            },
            {
                "name": "Junjie Wang",
                "institution": "Institute of Software at Chinese Academy of Sciences; University of Chinese Academy of Sciences",
                "country": "China"
            },
            {
                "name": "Song Wang",
                "institution": "York University",
                "country": "Canada"
            },
            {
                "name": "Nachiappan Nagappan",
                "institution": "Facebook",
                "country": "United States"
            }
        ]
    },
    {
        "title": "DeepScenario: An Open Driving Scenario Dataset for Autonomous Driving System Testing",
        "topics": "Data and Tool Showcase Track",
        "abstract": "With the rapid development of autonomous driving systems (ADSs), testing ADSs under various environmental conditions has become a key method to ensure the successful deployment of ADS in the real world. However, it is impossible to test all the scenarios due to the inherent complexity and uncertainty of ADSs and the driving tasks. Further, testing of ADSs is expensive regarding time and computational resources. Therefore, a large-scale driving scenario dataset consisting of various driving conditions is needed. To this end, we present an open driving scenario dataset $\\mathrm{DeepScenario}$, containing over 30\\textit{K} \\textit{executable} driving scenarios, which are collected by 2880 test executions of three driving scenario generation strategies. Each scenario in the dataset is labeled with six attributes characterizing test results. We further show the attribute statistics and distribution of driving scenarios. For example, there are 1050 collision scenarios, in 917 scenarios there were collisions with other vehicles, 105 and 28 with pedestrians and static obstacles, respectively. Target users include ADS developers who need to validate their systems under various environmental conditions.",
        "authors": [
            {
                "name": "Chengjie Lu",
                "institution": "Simula Research Laboratory and University of Oslo",
                "country": "Norway"
            },
            {
                "name": "Tao Yue",
                "institution": "Simula Research Laboratory",
                "country": "Norway"
            },
            {
                "name": "Shaukat Ali",
                "institution": "Simula Research Laboratory",
                "country": "Norway"
            }
        ]
    },
    {
        "title": "PTMTorrent: A Dataset for Mining Open-source Pre-trained Model Packages",
        "topics": "Data and Tool Showcase Track",
        "abstract": "Due to the cost of developing and training deep learning models from scratch, machine learning engineers have begun to reuse pre-trained models (PTMs) and fine-tune them for downstream tasks. PTM registries known as \u201cmodel hubs\u201d support engineers in distributing and reusing deep learning models. PTM packages include pre-trained weights, documentation, model architectures, datasets, and metadata. Mining the information in PTM packages will enable the discovery of engineering phenomena and tools to support software engineers. However, accessing this information is difficult \u2014 there are many PTM registries, and both the registries and the individual packages may have rate limiting for accessing the data. We present an open-source dataset, PTMTorrent, to facilitate the evaluation and understanding of PTM packages. This paper describes the creation, structure, usage, and limitations of the dataset. The dataset includes a snapshot of 5 model hubs and a total of 15,913 PTM packages. These packages are represented in a uniform data schema for cross-hub mining. We describe prior uses of this data and suggest research opportunities for mining using our dataset. %2F%7E%2F.The PTMTorrent dataset (v1) is available at:https://app.globus.org/file-manager?origin_id=55e17a6e-9d8f-11ed-a2a2-8383522b48d9&origin_path=Our dataset generation tools are available on GitHub:https://doi.org/10.5281/zenodo.7570357.",
        "authors": [
            {
                "name": "Wenxin Jiang",
                "institution": "Purdue University",
                "country": "United States"
            },
            {
                "name": "Nicholas Synovic",
                "institution": "Loyola University Chicago",
                "country": "United States"
            },
            {
                "name": "Purvish Jajal",
                "institution": "Purdue University",
                "country": null
            },
            {
                "name": "Taylor R. Schorlemmer",
                "institution": "Purdue University",
                "country": "United States"
            },
            {
                "name": "Arav Tewari",
                "institution": "Purdue University",
                "country": null
            },
            {
                "name": "Bhavesh Pareek",
                "institution": "Purdue University",
                "country": null
            },
            {
                "name": "George K. Thiruvathukal",
                "institution": "Loyola University Chicago and Argonne National Laboratory",
                "country": null
            },
            {
                "name": "James C. Davis",
                "institution": "Purdue University",
                "country": "United States"
            }
        ]
    },
    {
        "title": "Evaluating Software Documentation Quality",
        "topics": "Technical Papers",
        "abstract": "The documentation of software libraries is an essential resource for learning how to use the library. Bad documentation may demotivate a developer from using the library or may result in incorrect usage of the library. Therefore, as developers select which libraries to use and learn, it would be beneficial to know the quality of the available documentation. In this paper, we follow a systematic process to create an automatic documentation quality evaluation tool. We identify several documentation quality aspects from the literature and design metrics that measure these aspects. We design a documentation quality overview visualization to visualize and present these metrics, and receive intermediate feedback through a focused interview study. Based on the received feedback, we implement a web service that can evaluate a given documentation page for Java, JavaScript, and Python libraries.We use this web service to conduct a survey with 26 developers where we evaluate the usefulness of our metrics as well as whether they reflect developers\u2019 experiences when using this library. Our results show that participants rated most of our metrics highly, with Text Readability, and Code Readability (of examples) receiving the highest ratings. We also found several libraries where our evaluation reflected developers\u2019 experiences using the library, indicating the accuracy of our metrics.",
        "authors": [
            {
                "name": "Henry Tang",
                "institution": "University of Alberta",
                "country": "Canada"
            },
            {
                "name": "Sarah Nadi",
                "institution": "University of Alberta",
                "country": "Canada"
            }
        ]
    },
    {
        "title": "NICHE: A Curated Dataset of Engineered Machine Learning Projects in Python",
        "topics": "Data and Tool Showcase Track",
        "abstract": "Machine learning (ML) has gained much attention and been incorporated into our daily lives. While there are numerous publicly available ML projects on open source platforms such as GitHub, there have been limited attempts in filtering those projects to curate ML projects of high quality. The limited availability of such high-quality dataset poses an obstacle in understanding ML projects. To help clear this obstacle, we present NICHE, a manually labelled dataset consisting of 572 ML projects. Based on evidences of good software engineering practices, we label 441 of these projects as engineered and 131 as non-engineered. This dataset can help researchers understand the practices that are followed in high-quality ML projects. It can also be used as a benchmark for classifiers designed to identify engineered ML projects.",
        "authors": [
            {
                "name": "Ratnadira Widyasari",
                "institution": "Singapore Management University",
                "country": "Singapore"
            },
            {
                "name": "Zhou Yang",
                "institution": "Singapore Management University",
                "country": "Singapore"
            },
            {
                "name": "Ferdian Thung",
                "institution": "Singapore Management University",
                "country": "Singapore"
            },
            {
                "name": "Sheng Qin Sim",
                "institution": "Singapore Management University",
                "country": "Singapore"
            },
            {
                "name": "Fiona Wee",
                "institution": "Singapore Management University",
                "country": "Singapore"
            },
            {
                "name": "Camellia Lok",
                "institution": "Singapore Management University",
                "country": "Singapore"
            },
            {
                "name": "Jack Phan",
                "institution": "Singapore Management University",
                "country": "Singapore"
            },
            {
                "name": "Haodi Qi",
                "institution": "Singapore Management University",
                "country": "Singapore"
            },
            {
                "name": "Constance Tan",
                "institution": "Singapore Management University",
                "country": "Singapore"
            },
            {
                "name": "Qijin Tay",
                "institution": "Singapore Management University",
                "country": "Singapore"
            },
            {
                "name": "David Lo",
                "institution": "Singapore Management University",
                "country": "Singapore"
            }
        ]
    },
    {
        "title": "PICASO: Enhancing API Recommendations with Relevant Stack Overflow Posts",
        "topics": "Technical Papers",
        "abstract": "While having options could be liberating, too many options could lead to the sub-optimal solution being chosen. This is not an exception in the software engineering domain. Nowa- days, API has become imperative in making software developers\u2019 life easier. APIs help developers implement a function faster and more efficiently. However, given the large number of open- source libraries to choose from, choosing the right APIs is not a simple task. Previous studies on API recommendation leverage natural language (query) to identify which API would be suitable for the given task. However, these studies only consider one source of input, i.e., GitHub or Stack Overflow, independently. There are no existing approaches that utilize Stack Overflow to help generate better API sequence recommendations from queries obtained from GitHub. Therefore, in this study, we aim to provide a framework that could improve the result of the API sequence recommendation by leveraging information from Stack Overflow. In this work, we propose PICASO, which leverages a bi- encoder to do contrastive learning and a cross-encoder to build a classification model in order to find a semantically similar Stack Overflow given an annotation (i.e., code comment). Subsequently, PICASO then uses the Stack Overflow\u2019s title as a query expansion. PICASO then uses the extended queries to fine-tune a CodeBERT, resulting in an API sequence generation model. Based on our experiments, we found that incorporating the Stack Overflow title into CodeBERT would improve the performance of API sequence generation\u2019s BLEU-4 score by 10.8%.",
        "authors": [
            {
                "name": "Ivana Clairine Irsan",
                "institution": "Singapore Management University",
                "country": "Singapore"
            },
            {
                "name": "Ting Zhang",
                "institution": "Singapore Management University",
                "country": "Singapore"
            },
            {
                "name": "Ferdian Thung",
                "institution": "Singapore Management University",
                "country": "Singapore"
            },
            {
                "name": "Kisub Kim",
                "institution": "Singapore Management University",
                "country": "Singapore"
            },
            {
                "name": "David Lo",
                "institution": "Singapore Management University",
                "country": "Singapore"
            }
        ]
    },
    {
        "title": "What Do Users Ask in Open-Source AI Repositories? An Empirical Study of GitHub Issues",
        "topics": "Technical Papers",
        "abstract": "Arti\ufb01cial intelligence (AI) systems, which bene\ufb01t from the availability of large-scale datasets and increasing computational power, have become effective solutions to various critical tasks, such as natural language understanding, speech recognition, and image processing. The advancement of these AI systems are inseparable from open-source software (OSS). Specifically, many benchmarks, implementations, and frameworks for constructing AI systems are made open source and accessible to the general public, allowing researchers and practitioners to reproduce the reported results and broaden the application of AI systems. The development of AI systems follows a data-driven paradigm and is sensitive to hyperparameter settings and data separation. Developers may encounter unique problems when employing open-source AI repositories.",
        "authors": [
            {
                "name": "Zhou Yang",
                "institution": "Singapore Management University",
                "country": "Singapore"
            },
            {
                "name": "Chenyu Wang",
                "institution": "Singapore Management University",
                "country": "Singapore"
            },
            {
                "name": "Jieke Shi",
                "institution": "Singapore Management University",
                "country": "Singapore"
            },
            {
                "name": "Thong Hoang",
                "institution": "CSIRO's Data61",
                "country": null
            },
            {
                "name": "Pavneet Singh Kochhar",
                "institution": "Microsoft",
                "country": "Canada"
            },
            {
                "name": "Qinghua Lu",
                "institution": "CSIRO\u2019s Data61",
                "country": "Australia"
            },
            {
                "name": "Zhenchang Xing",
                "institution": null,
                "country": null
            },
            {
                "name": "David Lo",
                "institution": "Singapore Management University",
                "country": "Singapore"
            }
        ]
    },
    {
        "title": "Enabling Analysis and Reasoning on Software Systems through Knowledge Graph Representation",
        "topics": "Data and Tool Showcase Track",
        "abstract": "This work presents a knowledge-representation-based approach for analysing software systems. Its main components are: a generic and extensible knowledge model, and a knowledge extractor tool that generates instance-level knowledge graphs  from software repositories (currently Java). Our knowledge model can be used as a shared data-model in a software analysis pipeline. We illustrate the potential uses of our knowledge representation by performing experimental architecture recovery and identifying design pattern instance. We intend to use our ontology and extraction tool as a partial foundation for automated reasoning on software systems.",
        "authors": [
            {
                "name": "Satrio Adi Rukmono",
                "institution": null,
                "country": null
            },
            {
                "name": "Michel Chaudron",
                "institution": "Eindhoven University of Technology",
                "country": "Netherlands"
            }
        ]
    },
    {
        "title": "TypeScript's Evolution: An Analysis of Feature Adoption Over Time",
        "topics": "Technical Papers",
        "abstract": "TypeScript is a quickly evolving superset of JavaScript with active development of new features. Our paper seeks to understand how quickly these features are adopted by the developer community. Existing work in JavaScript shows the adoption of dynamic language features can be a major hindrance to static analysis. As TypeScript evolves the addition of features makes the underlying standard more and more difficult to keep up with. In our work we present an analysis of 500 open source TypeScript repositories and study the adoption of 13 language features over the past three years. We show that while new versions of the TypeScript compiler are aggressively adopted by the community the same cannot be said for language features. While some experience strong growth others are rarely adopted by projects. Our work serves as a starting point for future study of the adoption of features in TypeScript. We also release our analysis and data gathering software as open source in the hope it helps the programming languages community.",
        "authors": [
            {
                "name": "Joshua D. Scarsbrook",
                "institution": "The University of Queensland",
                "country": "Australia"
            },
            {
                "name": "Mark Utting",
                "institution": "The University of Queensland",
                "country": "Australia"
            },
            {
                "name": "Ryan K. L. Ko",
                "institution": "The University of Queensland",
                "country": "Australia"
            }
        ]
    },
    {
        "title": "DGMF: Fast Generation of Comparable, Updatable Dependency Graphs for Software Repositories",
        "topics": "Data and Tool Showcase Track",
        "abstract": "Dependency graphs for software repositories have been utilized in a variety of different research contexts. However, to this date there is no unified data model for such graphs, often prompting researchers to implement domain-specific methodologies from scratch. This greatly hinders comparability and makes it hard to incorporate existing tooling into new contexts.With this work we propose DGMF, a framework for mining dependency graphs via repository-specific, user-defined adapters. DGMF is designed to be fast, to require little repository-specific code, and to produce graphs that are comparable even across different repositories. We present our design and implementation, as well as three predefined adapters and an evaluation.",
        "authors": [
            {
                "name": "Tobias Litzenberger",
                "institution": "TU Dortmund University",
                "country": null
            },
            {
                "name": "Johannes D\u00fcsing",
                "institution": "TU Dortmund University",
                "country": "Germany"
            },
            {
                "name": "Ben Hermann",
                "institution": "TU Dortmund",
                "country": "Germany"
            }
        ]
    },
    {
        "title": "GIRT-Data: Sampling GitHub Issue Report Templates",
        "topics": "Data and Tool Showcase Track",
        "abstract": "GitHub\u2019s issue reports provide developers with valuable information that is essential to the evolution of a software development project. Contributors can use these reports to perform software engineering tasks like submitting bugs, requesting features, and collaborating on ideas. In the initial versions of issue reports, there was no standard way of using them. As a result, the quality of issue reports varied widely. To improve the quality of issue reports, GitHub introduced issue report templates (IRTs), which pre-fill issue descriptions when a new issue is opened. An IRT usually contains greeting contributors, describing project guidelines, and collecting relevant information. However, despite of effectiveness of this feature which was introduced in 2016, only nearly 5% of GitHub repositories (with more than 10 stars) utilize it. There are currently few articles on IRTs, and the available ones only consider a small number of repositories. In this work, we introduce GIRT-Data, the first and largest dataset of IRTs in both YAML and Markdown format. This dataset and its corresponding open-source crawler tool are intended to support research in this area and to encourage more developers to use IRTs in their repositories. The stable version of the dataset contains 1,084,300 repositories and 50,032 of them support IRTs. The stable version of the dataset and crawler is available here:https://github.com/kargaranamir/girt-data",
        "authors": [
            {
                "name": "Nafiseh Nikehgbal",
                "institution": "Sharif University of Technology",
                "country": null
            },
            {
                "name": "Amir Hossein Kargaran",
                "institution": "LMU Munich",
                "country": "Germany"
            },
            {
                "name": "Abbas Heydarnoori",
                "institution": "Bowling Green State University",
                "country": "United States"
            },
            {
                "name": "Hinrich Sch\u00fctze",
                "institution": "LMU Munich",
                "country": null
            }
        ]
    },
    {
        "title": "Feature Toggle Usage Patterns : A Case Study on Google Chromium",
        "topics": "Technical Papers",
        "abstract": "Feature toggles control the state of features and allow exposing unfinished features to a reduced cohort of users without affecting the general software operation. It is basically a variable used in if conditions to control the flow of program execution. Since there is no universal standard of using feature toggles established yet, developers write code around feature toggles and use them spontaneously. Certain usage patterns of feature toggles may even lead to code smells. In this short paper we introduce six different toggle usage patterns from Google Chromium and discuss the possible reasons, consequences, and detection methods. We further conduct a mixed-method approach to analyze them. Since this study is still in progress, we report our early results only for the three most commonly appeared usage patterns. We validate our quantitative findings with the qualitative results obtained by interviewing 15 Google developers. We found that there are 3.1K toggles present in 38 components of Chromium. In the median case, nested toggles are shared by 5 different files, spread toggles span 2 different components, and dead toggles cover an average of 4 lines of code (loc). Novel aspects:- Although usage patterns of C pre-processors (#ifdef s) are studied in the past, we did not find any study particularly focusing on the run-time feature toggles usage patterns. Hence, we have been inspired to do an exploratory study to investigate different usage patterns of feature toggles. We chose Google Chromium as a case study at this point since Google developers use feature toggles extensively.",
        "authors": [
            {
                "name": "Md Tajmilur Rahman",
                "institution": "Gannon University",
                "country": "United States"
            }
        ]
    },
    {
        "title": "On Codex Prompt Engineering for OCL Generation: An Empirical Study",
        "topics": "Technical Papers",
        "abstract": "The Object Constraint Language (OCL) is a declar- ative language that provides constraint and object query expres- sions on any MOF model. OCL can be used to add precision and conciseness to the UML models. Despite its advantages, the unfamiliar syntax of OCL contributed to its lower adoption by software practitioners. This paper experiments with prompt engineering and Large Language Models (LLM) for OCL con- straint generation from natural language. LLMs, such as GPT-3, have achieved substantial gains in many NLP tasks, including text generation and semantic parsing. Similarly, researchers have improved on downstream tasks by fine-tuning the LLMs for the target task. Codex, a GPT-3 descendant by OpenAI, is fine-tuned on publicly available code from GitHub and has proven the ability to generate code in many programming languages, powering the AI-pair programmer Copilot. One way to exploit Codex is to engineer prompts for the downstream task. In this paper, we investigate the reliability of the OCL constraints generated by Codex, given the specification in natural language. To accomplish this, we collected a dataset of 15 UML models with 169 specifica- tions and followed a prompt engineering approach. We manually crafted a template with slots to fill in the UML information and task description, following the prefix shape to complete the template with the generated OCL constraint. Both zero- and few-shot learning methods were adopted in the experiments. The evaluation is reported by measuring the syntactic validity and the execution accuracy scores of the generated OCL constraints. Our findings suggest that by enriching the prompts with the UML information of the model and with influence from the correct behaviour of the few-shot examples, the reliability of the generated OCL constraints increases. In addition, we also investigated their similarity by calculating the cosine metric between the correctly generated OCL constraints and their corresponding human-written ground truth. The results indicate that, on average, the generated OCL constraints were similar to their ground truth.",
        "authors": [
            {
                "name": "Seif Abukhalaf",
                "institution": "Polytechnique Montreal",
                "country": "Canada"
            },
            {
                "name": "Mohammad Hamdaqa",
                "institution": "Polytechnique Montr\u00e9al",
                "country": "Canada"
            },
            {
                "name": "Foutse Khomh",
                "institution": "Polytechnique Montr\u00e9al",
                "country": "Canada"
            }
        ]
    },
    {
        "title": "Wasmizer: Curating WebAssembly-driven Projects on GitHub",
        "topics": "Technical Papers",
        "abstract": "WebAssembly is increasingly being used as a portable compilation target for high-level programming lan- guages. However, the current datasets of WebAssembly programs only include binaries without their corresponding source code. Having the source code available along with the binaries would be helpful for tool writers, and it would enable in-depth program analyses. We mined GitHub and collected 2540 C and C++ projects that are highly-related to WebAssembly. From these projects, we extracted a dataset of 8915 binaries that belong to 572 projects and linked WebAssembly binaries to their source code. To demonstrate an application of this dataset, we investigated the presence of eight WebAssembly compilation smells in a subset of these projects. We deployed Wasmizer, a tool that regularly mines GitHub projects and makes an up-to- date dataset of WebAssembly sources and their binaries publicly available",
        "authors": [
            {
                "name": "Alexander Nicholson",
                "institution": "University of Auckland",
                "country": null
            },
            {
                "name": "Quentin Sti\u00e9venart",
                "institution": "Vrije Universiteit Brussel",
                "country": "Belgium"
            },
            {
                "name": "Arash Mazidi",
                "institution": "TU Clausthal",
                "country": "Germany"
            },
            {
                "name": "Mohammad Ghafari",
                "institution": "TU Clausthal",
                "country": "Germany"
            }
        ]
    },
    {
        "title": "Enriching Source Code with Contextual Data for Code Completion Models: An Empirical Study",
        "topics": "Technical Papers",
        "abstract": "Abstract\u2014Transformer-based pre-trained models have recently achieved great results in solving many software engineering tasks including automatic code completion which is a staple in a developer\u2019s toolkit. While many have striven to improve the code- understanding abilities of such models, the opposite \u2013 making the code easier to understand \u2013 has not been properly investigated. In this study, we aim to answer whether making code easier to understand through using contextual data improves the perfor- mance of pre-trained code language models for the task of code completion. We consider type annotations and comments as two common forms of additional contextual information that often help developers understand code better. For the experiments, we study code completion in two granularity levels; token and line completion and take three recent and large-scale language models for source code: UniXcoder, CodeGPT, and InCoder with five evaluation metrics. Finally, we perform the Wilcoxon Signed Rank test to gauge significance and measure the effect size. Contrary to our expectations, all models perform better if type annotations are removed (albeit the effect sizes are small). For comments, we find that the models perform better in the presence of multi-line comments (again with small effect sizes). Based on our observations, we recommend making proper design choices when training, fine-tuning, or simply selecting such models given the intended data and application. Better evaluations and multi- modal techniques can also be further investigated to improve the practicality and accuracy of auto-completions.",
        "authors": [
            {
                "name": "Tim van Dam",
                "institution": "Delft University of Technology",
                "country": "Netherlands"
            },
            {
                "name": "Maliheh Izadi",
                "institution": "Delft University of Technology",
                "country": "Netherlands"
            },
            {
                "name": "Arie van Deursen",
                "institution": "Delft University of Technology",
                "country": "Netherlands"
            }
        ]
    },
    {
        "title": "microSecEnD: A Dataset of Security-Enriched Dataflow Diagrams for Microservice Applications",
        "topics": "Data and Tool Showcase Track",
        "abstract": "Dataflow diagrams (DFDs) are useful resources in securing applications since they show a software system\u2019s architecture and allow assessing architectural security and weaknesses. Enriching them with annotations about implemented security features further strengthens this ability. This is especially true for microservice applications, as their most pressing security concerns stem from their separation into multiple services. Researchers need data to work on these issues and enhance microservices\u2019 architectural security. In this work, we present microSecEnD, a dataset of 17 manually created DFDs that are extensively annotated with information on implemented security features. We provide traceability for all model items. Further, a mapping to a list of 17 architectural security best-practices is provided. Finally, for each best-practice that an application violates, we present a model variant that does adhere to it.",
        "authors": [
            {
                "name": "Simon Schneider",
                "institution": "Hamburg University of Technology",
                "country": "Germany"
            },
            {
                "name": "Tufan \u00d6zen",
                "institution": "Hamburg University of Technology",
                "country": "Germany"
            },
            {
                "name": "Michael Chen",
                "institution": "Hamburg University of Technology",
                "country": "Germany"
            },
            {
                "name": "Riccardo Scandariato",
                "institution": "Hamburg University of Technology",
                "country": "Germany"
            }
        ]
    },
    {
        "title": "Model-Agnostic Syntactical Information for Pre-Trained Programming Language Models",
        "topics": "Technical Papers",
        "abstract": "Abstract\u2014Pre-trained Programming Language Models (PPLMs) achieved many recent states of the art results for many code-related software engineering tasks. Though some studies use data flow or propose tree-based models that utilize Abstract Syntax Tree (AST), most PPLMs do not fully utilize the rich syntactical information in source code. Still, the input is considered a sequence of tokens. There are two issues; the first is computational inefficiency due to the quadratic relationship between input length and attention complexity. Second, any syntactical information, when needed as an extra input to the current PPLMs, requires the model to be pre-trained from scratch, wasting all the computational resources already used for pre-training the current models. In this work, we propose Named Entity Recognition (NER) adapters, lightweight modules that can be inserted into Transformer blocks to learn type information extracted from the AST. These adapters can be used with current PPLMs such as CodeBERT, GraphCodeBERT, and CodeT5. We train the NER adapters using a novel Token Type Classification objective function (TTC). We insert our proposed work in CodeBERT, building CodeBERTER, and evaluate the performance on two tasks of code refinement and code summarization. CodeBERTER improves the accuracy of code refinement from 16.4 to 17.8 while using 80% of training parameter budget compared to the fully fine-tuning approach, and the BLEU score of code summarization from 14.75 to 15.90 while reducing 77% of training parameters compared to the fully fine-tuning approach.",
        "authors": [
            {
                "name": "Iman Saberi",
                "institution": "University of British Columbia Okanagan",
                "country": "Canada"
            },
            {
                "name": "Fatemeh Hendijani Fard",
                "institution": "University of British Columbia",
                "country": "Canada"
            }
        ]
    },
    {
        "title": "What Happens When We Fuzz? Investigating OSS-Fuzz Bug History",
        "topics": "Technical Papers",
        "abstract": "BACKGROUND: Software engineers must be vigilant in preventing and correcting vulnerabilities and other critical bugs. In servicing this need, numerous tools and techniques have been developed to assist developers. Fuzzers, by autonomously generating inputs to test programs, promise to save time by detecting memory corruption, input handling, exception cases, and other issues.",
        "authors": [
            {
                "name": "Brandon Keller",
                "institution": "Rochester Institute of Technology",
                "country": "United States"
            },
            {
                "name": "Benjamin S. Meyers",
                "institution": "Rochester Institute of Technology",
                "country": "United States"
            },
            {
                "name": "Andrew Meneely",
                "institution": "Rochester Institute of Technology",
                "country": "United States"
            }
        ]
    },
    {
        "title": "Cross-Domain Evaluation of a Deep Learning-Based Type Inference System",
        "topics": "Technical Papers",
        "abstract": "Optional type annotations allow for enriching dynamic programming languages with static typing features like better Integrated Development Environment (IDE) support, more precise program analysis, and early detection and prevention of type-related runtime errors. Machine learning-based type inference promises interesting results for automating this task. However, the practical usage of such systems depends on their ability to generalize across different domains, as they are often applied outside their training domain.In this work, we investigate Type4Py as a representative of state-of-the-art deep learning-based type inference systems, by conducting extensive cross-domain experiments. Thereby, we address the following problems: class imbalances, out-of-vocabulary words, dataset shifts, and unknown classes. To perform such experiments, we use the datasets ManyTypes4Py and CrossDomainTypes4Py. The latter we introduce in this paper. Our dataset enables the evaluation of type inference systems in different domains of software projects and has over 1,000,000 type annotations mined on GitHub and Libraries. It consists of data from the two domains web development and scientific calculation. Through our experiments, we detect that the shifts in the dataset and the long-tailed distribution with many rare and unknown data types decrease the performance of the deep learning-based type inference system drastically. In this context, we test unsupervised domain adaptation methods and fine-tuning to overcome these issues. Moreover, we investigate the impact of out-of-vocabulary words.",
        "authors": [
            {
                "name": "Bernd Gruner",
                "institution": "DLR Institute of Data Science",
                "country": null
            },
            {
                "name": "Tim Sonnekalb",
                "institution": "German Aerospace Center (DLR)",
                "country": "Germany"
            },
            {
                "name": "Thomas S. Heinze",
                "institution": "Cooperative University Gera-Eisenach",
                "country": null
            },
            {
                "name": "Clemens-Alexander Brust",
                "institution": "German Aerospace Center (DLR)",
                "country": "Germany"
            }
        ]
    },
    {
        "title": "An Empirical Study on the Performance of Individual Issue Label Prediction",
        "topics": "Technical Papers",
        "abstract": "In GitHub, open-source software (OSS) developers label issue reports. As issue labeling is a labor-intensive manual task, automatic approaches have developed to label issue reports. However, those approaches have shown a limited performance. Therefore, it is necessary to analyze the performance of predicting labels for an issue report. Understanding the labels with high performance and those with low performance can help improve the performance of automatic issue labeling tasks. In this paper, we investigate the performance of individual label prediction. Our investigation uncovers labels with high performance and those with low performance. Our results can help researchers to understand the different characteristics of labels and help the developer to develop a unified approach that combines several effective approaches for different kinds of issues.",
        "authors": [
            {
                "name": "Jueun Heo",
                "institution": null,
                "country": null
            },
            {
                "name": "Seonah Lee",
                "institution": "Gyeongsang National University",
                "country": "South Korea"
            }
        ]
    },
    {
        "title": "Tutorial: Recognizing Developers' Emotions Using Non-invasive Biometrics Sensors",
        "topics": "Tutorials",
        "abstract": "Grounding on recent research in this field, this tutorial will provide attendees with an overview on how to leverage biometrics for recognizing cognitive and affective states of software developers. Software development is an intellectual activity requiring creativity and problem-solving skills, which are known to be influenced by emotions. Developers experience a wide range of affective states during programming tasks, which may have an impact on their job performance and wellbeing. Early recognition of negative emotions, such as stress or frustration can enable just-in-time intervention for developers and team managers, in order to prevent burnout and undesired turnover. Attendees will have the possibility to familiarize with non-invasive biometric sensors for measuring that can be comfortably worn while programming. We will learn how to collect biometric data during software development activities, how to preprocess them and how to extract features to be used for supervised training of emotion classifiers.",
        "authors": [
            {
                "name": "Nicole Novielli",
                "institution": "University of Bari",
                "country": "Italy"
            }
        ]
    },
    {
        "title": "HasBugs - Handpicked Haskell Bugs",
        "topics": "Data and Tool Showcase Track",
        "abstract": "We present HasBugs, a manually collected Dataset of 25 Haskell Bugs from 6 Open Source Repositories. We provide buggy, tested and fixed versions as well as reproduction packages and a summary of the bug and its context. For technical users,the dataset is meant to either help researchers adopt techniques from other Languages for Haskell or to provide a human-verified gold standard for tool-evaluation. We also see applicability for qualitative research, e.g. by analysis of bug-lifecycles and comparison to other languages. We provide a companion website for easy access and overview underhttps://ciselab.github.io/HasBugs/",
        "authors": [
            {
                "name": "Leonhard Applis",
                "institution": "Delft University of Technology",
                "country": "Netherlands"
            },
            {
                "name": "Annibale Panichella",
                "institution": "Delft University of Technology",
                "country": "Netherlands"
            }
        ]
    },
    {
        "title": "Semantically-enriched Jira Issue Tracking Data",
        "topics": "Data and Tool Showcase Track",
        "abstract": "Current state of practice dictates that software developers host their projects online and employ project management systems to monitor the development of product features, keep track of bugs, and prioritize task assignments. The data stored in these systems, if their semantics are extracted effectively, can be used to answer several interesting questions, such as finding who is the most suitable developer for a task, what the priority of a task should be, or even what is the actual workload of the software team. To support researchers and practitioners that work towards these directions, we have built a system that crawls data from the Jira management system, performs topic modeling on the data to extract useful semantics and stores them in a practical database schema. We have used our system to retrieve and analyze 656 projects of the Apache Software Foundation, comprising data from more than a million Jira issues.",
        "authors": [
            {
                "name": "Themistoklis Diamantopoulos",
                "institution": "Electrical and Computer Engineering Dept, Aristotle University of Thessaloniki",
                "country": "Greece"
            },
            {
                "name": "Dimitrios-Nikitas Nastos",
                "institution": "Electrical and Computer Engineering Dept., Aristotle University of Thessaloniki",
                "country": null
            },
            {
                "name": "Andreas Symeonidis",
                "institution": "Electrical and Computer Engineering Dept., Aristotle University of Thessaloniki",
                "country": null
            }
        ]
    },
    {
        "title": "An exploratory study of bug introducing changes: what happens when bugs are introduced in open source software?",
        "topics": "Registered Reports",
        "abstract": "Many studies consider the relation between individual aspects and bug introduction, e.g., software testing and code review. Due to the design of the studies the results are usually only about correlations as interactions or interventions are not considered.",
        "authors": [
            {
                "name": "Lukas Schulte",
                "institution": "Universitity of Passau",
                "country": null
            },
            {
                "name": "Anamaria Mojica-Hanke",
                "institution": "University of Passau and Universidad de los Andes",
                "country": null
            },
            {
                "name": "Mario Linares-Vasquez",
                "institution": "Universidad de los Andes",
                "country": "Colombia"
            },
            {
                "name": "Steffen Herbold",
                "institution": "University of Passau",
                "country": "Germany"
            }
        ]
    },
    {
        "title": "An Empirical Study of High Performance Computing (HPC) Performance Bugs",
        "topics": "Technical Papers",
        "abstract": "Performance efficiency and scalability are the major design goals for high performance computing (HPC) applications. However, it is challenging to achieve high efficiency and scalability for such applications due to complex underlying hardware architecture, inefficient algorithm implementation, suboptimal code generation by the compilers, inefficient parallelization, and so on. As a result, the HPC community spends a significant effort detecting and fixing the performance bugs frequently appearing in scientific applications. However, it is important to accumulate the experience to guide the scientific software engineering community to write performance-efficient code.",
        "authors": [
            {
                "name": "Md Abul Kalam Azad",
                "institution": "University of Michigan - Dearborn",
                "country": "United States"
            },
            {
                "name": "Nafees Iqbal",
                "institution": "University of Michigan - Dearborn",
                "country": "United States"
            },
            {
                "name": "Foyzul Hassan",
                "institution": "University of Michigan - Dearborn",
                "country": "United States"
            },
            {
                "name": "Probir Roy",
                "institution": "University of Michigan at Dearborn",
                "country": "United States"
            }
        ]
    },
    {
        "title": "Investigating the Resolution of Vulnerable Dependencies with Dependabot Security Updates",
        "topics": "Technical Papers",
        "abstract": "Modern software development practices increasingly rely on third-party libraries due to the inherent benefits of reuse. However, libraries may contain security vulnerabilities that can propagate to the dependent applications. To counter this, maintainers of dependent projects should monitor their dependencies and security reports to ensure that only patched releases of the upstream applications are in use. As manual maintenance of dependencies has shown to be ineffective, several automated tools (aka bots) have been proposed to assist developers in rapidly identifying and resolving vulnerable dependencies. In this work, we focus on Dependabot, a popular bot providing security and version updates, and study developers\u2019 receptivity to its security updates in mature and actively maintained JavaScript projects. Moreover, we carry out a fine-grained analysis of the lifecycle of every vulnerability to manifest how they are dealt with in the presence of Dependabot. Our findings show that the task of fixing vulnerable dependencies is, to a large extent, delegated to Dependabot and that developers merge the majority of security updates within several days. On the other hand, when developers do not merge a security update, they usually address the identified vulnerability manually. This approach, however, often takes up to several months which in turn could expose the projects to security issues.",
        "authors": [
            {
                "name": "Hamid Mohayeji Nasrabadi",
                "institution": "Eindhoven University of Technology",
                "country": "Netherlands"
            },
            {
                "name": "Andrei Agaronian",
                "institution": "Eindhoven University of Technology",
                "country": "Netherlands"
            },
            {
                "name": "Eleni Constantinou",
                "institution": "University of Cyprus",
                "country": "Cyprus"
            },
            {
                "name": "Nicola Zanone",
                "institution": "Eindhoven University of Technology",
                "country": "Netherlands"
            },
            {
                "name": "Alexander Serebrenik",
                "institution": "Eindhoven University of Technology",
                "country": "Netherlands"
            }
        ]
    },
    {
        "title": "Are We Speeding Up or Slowing Down? On Temporal Aspects of Code Velocity",
        "topics": "Technical Papers",
        "abstract": "This paper investigates how the duration of various code review periods changes over a projects\u2019 lifetime. We study four open-source software (OSS) projects: Blender, FreeBSD, LLVM, and Mozilla. We mine and analyze the characteristics of 283,235 code reviews that cover, on average, seven years\u2019 worth of development. Our main conclusion is that neither the passage of time or the project\u2019s size impact code velocity. We find that (a) the duration of various code review periods (time-to-first-response, time-to-accept, and time-to-merge) for FreeBSD, LLVM, and Mozilla either becomes shorter or stays the same; no directional trend is present for Blender, (b) an increase in the size of the code bases (annually 3\u201317%) does not accompany a decrease in code velocity, and (c) for FreeBSD, LLVM, and Mozilla, the 30-day moving median stays in a fixed range for time-to-merge. These findings do not change with variabilities in code churn metrics, such as the number of commits or distinct authors of code changes.",
        "authors": [
            {
                "name": "Gunnar Kudrjavets",
                "institution": "University of Groningen",
                "country": "United States"
            },
            {
                "name": "Nachiappan Nagappan",
                "institution": "Facebook",
                "country": "United States"
            },
            {
                "name": "Ayushi Rastogi",
                "institution": "University of Groningen, The Netherlands",
                "country": "Netherlands"
            }
        ]
    },
    {
        "title": "Unveiling the Relationship Between Continuous Integration and Code Coverage",
        "topics": "Technical Papers",
        "abstract": "Continuous integration (CI) is a software engineering practice that advocates the frequent integration of software through an automated build process. Existing research has explored the benefits of CI, such as detecting errors earlier in the software life-cycle. Although CI places a heavy focus on automated tests, it is still not clear whether CI is associated with better code coverage, which could be a major benefit of using CI. To investigate whether CI is associated with an improvement in code coverage, our work compares 30 projects that adopted CI (CI projects) and 30 projects that have never adopted CI (NOCI projects) to investigate the relationship between the evolution of code coverage and the adoption of CI. We observe that CI projects have more rising code coverage trends (50%) when compared to NOCI projects (10%). Additionally, maintaining trends differ between CI and NOCI projects; CI projects tend to stabilize their code coverage at a higher coverage value when compared to NOCI projects. We study the types of code changes that affect the coverage levels. We identify that projects that adopted CI substantially increased the number of code changes that rised code coverage in the projects. Finally, our work reveals a positive association between CI and better code coverage.",
        "authors": [
            {
                "name": "Jos\u00e9 Diego Saraiva da Silva",
                "institution": "UFRN",
                "country": null
            },
            {
                "name": "Daniel Alencar Da Costa",
                "institution": "University of Otago",
                "country": "New Zealand"
            },
            {
                "name": "Uir\u00e1 Kulesza",
                "institution": "Federal University of Rio Grande do Norte",
                "country": "Brazil"
            },
            {
                "name": "Gustavo Siz\u00edlio",
                "institution": "Federal University of Rio Grande do Norte",
                "country": "Brazil"
            },
            {
                "name": "Jos\u00e9 Gameleira Neto",
                "institution": "Federal University of Rio Grande do Norte",
                "country": "Brazil"
            },
            {
                "name": "Roberta Coelho",
                "institution": null,
                "country": null
            },
            {
                "name": "Mei Nagappan",
                "institution": "University of Waterloo",
                "country": "Canada"
            }
        ]
    },
    {
        "title": "The Atlassian Data Lake: consolidating enriched software development data in a single, queryable system",
        "topics": "Industry Track",
        "abstract": "Managing work within and across multiple software teams requires a high level of visibility into the work of those teams, to inform decisions on team velocity, resource allocation, and return on investment. Since much of the work is conducted in software development tools, they are an essential source for consolidating and presenting a clear picture of the work being conducted. We describe the Atlassian Data Lake, a solution that contains cross-product and cross-site data for easy analysis with pre-modeled and enriched fields to speed up insight generation. The Atlassian Data Lake enables practitioners and researchers to access software development tools data and derive software analytics without the overhead typically associated with data retrieval and post-processing.",
        "authors": [
            {
                "name": "Arik Friedman",
                "institution": "Atlassian",
                "country": "Australia"
            },
            {
                "name": "Rohan Dhupelia",
                "institution": "Atlassian",
                "country": "Australia"
            },
            {
                "name": "Ben Jackson",
                "institution": "Atlassian",
                "country": "Australia"
            }
        ]
    },
    {
        "title": "EGAD: A Moldable Tool for GitHub Action Analysis",
        "topics": "Data and Tool Showcase Track",
        "abstract": "GitHub Actions (GA) enjoy increasing popularity in many software development projects as a means to automate repetitive software engineering tasks by enabling programmable event-driven workflows. Researchers and developers typically analyze GA at the raw data level using batch tools to mine and analyze actions, jobs, and steps within GA workflows. Although this approach is widely applicable, it ignores the specific context of the GA workflow domain. Consequently, researchers and developers do not reason directly about the domain abstractions.",
        "authors": [
            {
                "name": "Pablo Valenzuela-Toledo",
                "institution": "University of Bern",
                "country": "Switzerland"
            },
            {
                "name": "Alexandre Bergel",
                "institution": "University of Chile",
                "country": "Chile"
            },
            {
                "name": "Timo Kehrer",
                "institution": "University of Bern",
                "country": "Switzerland"
            },
            {
                "name": "Oscar Nierstrasz",
                "institution": "University of Bern, Switzerland",
                "country": "Switzerland"
            }
        ]
    },
    {
        "title": "An Exploratory Study on Energy Consumption of Dataframe Processing Libraries",
        "topics": "Technical Papers",
        "abstract": "The energy consumption of machine learning applications and their impact on the environment has recently gained attention as a research area, focusing on the model creation and training/inference phases. However, the data-oriented stages of the machine learning pipeline, which involve preprocessing, cleaning, and exploratory analysis, are critical components. However, energy consumption during these stages has received limited attention. To fill this gap, as a first step, we aim to investigate the energy consumption of three popular dataframe processing libraries, namely Pandas, Vaex, and Dask. We perform experiments across 21 dataframe processing operations within four categories, utilizing three distinct datasets. Our results indicate that no single library is the most energy efficient for all tasks, and the choice of a library can have a significant impact on energy consumption based on the types and frequencies of operations performed. The findings of this study suggest the potential for optimization of the energy consumption of data-oriented stages in the machine learning pipeline and warrant further research in this area.",
        "authors": [
            {
                "name": "Shriram Shanbhag",
                "institution": "IIT Tirupati",
                "country": "India"
            },
            {
                "name": "Sridhar Chimalakonda",
                "institution": "IIT Tirupati",
                "country": "India"
            }
        ]
    },
    {
        "title": "Energy Consumption Estimation of API-usage in Mobile Apps via Static Analysis",
        "topics": "Technical Papers",
        "abstract": "Smartphone application (app) developers measure or estimate the energy consumption of their apps to ensure that they are not consuming too much energy before releasing them to the end-users. However, existing measurement and estimation techniques are cumbersome because they require developers to generate test cases and execute them on an expensive, sophisticated hardware. To address these challenges, we have proposed a static-analysis based approach that estimates the energy consumption of API usage in an app, and eliminates the need for generating and executing test cases. To instantiate our approach, we have created micro-benchmarks for the Swift SQLite API and measured the energy profile of its select, insert, and update operations. Given a Swift app, we first scan it for uses of SQLite. We then combine that information with the measured energy profile to compute E-factor, an estimate of the energy consumption of the API usage in an app. To showcase the viability of using E-factor in practice, we calculate the E-factor of 56 real-world iOS apps, and compare 16 versions and 11 methods to their hardware-based energy measurements. Our findings show that E-factor has a positive correlation with the hardware-based energy measurements. This result indicates that E-factor can be used as an estimate to compare the energy consumption difference in API usage across the different versions of an app. Additionally, developers can use E-factor to identify which methods within their apps have the highest energy consumption and focus on optimizing those methods. Our approach is most useful in an Integrated Development Environment (IDE) or Continuous Integration/Continuous Deployment (CI/CD) pipeline, where developers can receive warnings about high energy consumption in their app within milliseconds of making a modification to their code.",
        "authors": [
            {
                "name": "Abdul Ali Bangash",
                "institution": "University of Alberta",
                "country": "Canada"
            },
            {
                "name": "Qasim Jamal",
                "institution": "FAST National University",
                "country": null
            },
            {
                "name": "Kalvin Eng",
                "institution": "University of Alberta",
                "country": null
            },
            {
                "name": "Karim Ali",
                "institution": "University of Alberta",
                "country": null
            },
            {
                "name": "Abram Hindle",
                "institution": "University of Alberta",
                "country": null
            }
        ]
    },
    {
        "title": "Understanding issues related to personal data and data protection in open source projects on GitHub",
        "topics": "Registered Reports",
        "abstract": "Data protection regulations such as the GDPR and the CCPA affect how software may handle the personal data of its users and how consent for handling of such data may be given. Prior literature focused on how this works in operation, but lacks a perspective of the impact on the software development process.",
        "authors": [
            {
                "name": "Anne Hennig",
                "institution": "Karlsruhe Institute of Technology",
                "country": null
            },
            {
                "name": "Lukas Schulte",
                "institution": "Universitity of Passau",
                "country": null
            },
            {
                "name": "Steffen Herbold",
                "institution": "University of Passau",
                "country": "Germany"
            },
            {
                "name": "Oksana Kulyk",
                "institution": "IT University of Copenhagen, Denmark",
                "country": "Denmark"
            },
            {
                "name": "Peter Mayer",
                "institution": "University of Southern Denmark",
                "country": null
            }
        ]
    },
    {
        "title": "Whistleblowing and Tech on Twitter",
        "topics": "Technical Papers",
        "abstract": "From airports to banks, healthcare, space crafts, and even amazon services, technology impacts almost every aspect of today\u2019s life. If wrongdoings occur within or in relation to technology, they can have big implications on individuals, groups of people, or society as a whole. Whistleblowers are insiders who expose such wrongdoings\u2014 eventually stopping misconducts, such as fraud, endangerment to public health and safety, or damage to the environment. Twitter is a microblogging service that allows millions of users to share their views with people distributed all over the world on a daily basis. Tweets have the potential to contain useful information about whistleblowing in tech, from the general public and whistleblowers. However, until now this point has not been researched. To fill this gap, we conducted an exploratory study on technology-related whistleblowing tweets by manually analysing tweets, utilising descriptive statistics, and machine learning techniques. We mined 7,400 of tweets from whistleblowers themselves, as well as news and opinions about certain whistleblowers and whistleblowing cases. Although our results show that only 30% of the tweets in our sample dataset (obtained through specific search terms) contained relevant information about whistleblowing in technology, our analysis shows that tweets provide valuable information for both researchers and companies to understand the public opinion regarding whistleblowing cases. Furthermore, we found that machine learning techniques are promising means for extracting information about whistleblowing in tech from the vast stream of tweets.",
        "authors": [
            {
                "name": "Laura Duits",
                "institution": "Vrije Universiteit Amsterdam",
                "country": "Netherlands"
            },
            {
                "name": "Isha Kashyap",
                "institution": "Vrije Universiteit Amsterdam",
                "country": "Netherlands"
            },
            {
                "name": "Joey Bekkink",
                "institution": "Vrije Universiteit Amsterdam",
                "country": "Netherlands"
            },
            {
                "name": "Kousar Aslam",
                "institution": "Vrije Universiteit Amsterdam",
                "country": "Netherlands"
            },
            {
                "name": "Emitz\u00e1 Guzm\u00e1n",
                "institution": "Vrije Universiteit Amsterdam",
                "country": "Netherlands"
            }
        ]
    },
    {
        "title": "SecretBench: A Dataset of Software Secrets",
        "topics": "Data and Tool Showcase Track",
        "abstract": "According to GitGuardian\u2019s monitoring of public GitHub repositories, the exposure of secrets (API keys and other credentials) increased two-fold in 2021 compared to 2020, totaling more than six million secrets. However, no benchmark dataset is publicly available for researchers and tool developers to evaluate secret detection tools that produce many false positive warnings. The goal of our paper is to aid researchers and tool developers in evaluating and improving secret detection tools by curating a benchmark dataset of secrets through a systematic collection of secrets from open-source repositories. We present a labeled dataset of source codes containing 97,479 secrets (of which 15,084 are true secrets) of various secret types extracted from 818 public GitHub repositories. The dataset covers 49 programming languages and 311 file types.",
        "authors": [
            {
                "name": "Setu Kumar Basak",
                "institution": "North Carolina State University",
                "country": "United States"
            },
            {
                "name": "Lorenzo Neil",
                "institution": "North Carolina State University",
                "country": "United States"
            },
            {
                "name": "Bradley Reaves",
                "institution": "North Carolina State University",
                "country": "United States"
            },
            {
                "name": "Laurie Williams",
                "institution": "North Carolina State University",
                "country": "United States"
            }
        ]
    },
    {
        "title": "UNGOML: Automated Classification of unsafe Usages in Go",
        "topics": "Technical Papers",
        "abstract": "The Go programming language offers strong protection from memory corruption. As an escape hatch of these protections, it provides the unsafe package. Previous studies identified that this unsafe package is frequently used in real-world code for several purposes, e.g., serialization or casting types. Due to the variety of these reasons, it may be possible to refactor specific usages to avoid potential vulnerabilities. However, the classification of unsafe usages is challenging and requires the context of the call and the program\u2019s structure. In this paper, we present the first automated classifier for unsafe usages in Go, UNGOML, to identify what is done with the unsafe package and why it is used. For UNGOML, we built four custom deep-learning classifiers trained on a manually labeled data set. We represent Go code as enriched control-flow graphs (CFGs) and solve the label prediction task with one single-vertex and three context-aware classifiers. All three context-aware classifiers achieve a top-1 accuracy of more than 86% for both dimensions, WHAT and WHY. Furthermore, in a set-valued conformal prediction setting, we achieve accuracies of more than 93% with mean label set sizes of 2 for both dimensions. Thus, UNGOML can be used to efficiently filter unsafe usages for use cases such as refactoring or a security audit.",
        "authors": [
            {
                "name": "Anna-Katharina Wickert",
                "institution": "TU Darmstadt",
                "country": "Germany"
            },
            {
                "name": "Clemens Damke",
                "institution": "University of Munich (LMU)",
                "country": "Germany"
            },
            {
                "name": "Lars Baumg\u00e4rtner",
                "institution": "Technische Universit\u00e4t Darmstadt",
                "country": "Germany"
            },
            {
                "name": "Eyke H\u00fcllermeier",
                "institution": "University of Munich (LMU)",
                "country": "Germany"
            },
            {
                "name": "Mira Mezini",
                "institution": "TU Darmstadt",
                "country": "Germany"
            }
        ]
    },
    {
        "title": "Connecting the .dotfiles: Checked-In Secret Exposure with Extra (Lateral Movement) Steps",
        "topics": "Technical Papers",
        "abstract": "[Background] Personal software configurations, known as \\emph{dotfiles}, are increasingly being shared in public repositories.",
        "authors": [
            {
                "name": "Gerhard Jungwirth",
                "institution": "TU Wien",
                "country": "Austria"
            },
            {
                "name": "Aakanksha Saha",
                "institution": "TU Wien",
                "country": "Austria"
            },
            {
                "name": "Michael Schr\u00f6der",
                "institution": "TU Wien",
                "country": "Austria"
            },
            {
                "name": "Tobias Fiebig",
                "institution": "Max-Planck-Institut f\u00fcr Informatik",
                "country": null
            },
            {
                "name": "Martina Lindorfer",
                "institution": "TU Wien",
                "country": "Austria"
            },
            {
                "name": "J\u00fcrgen Cito",
                "institution": "TU Wien",
                "country": "Austria"
            }
        ]
    },
    {
        "title": "MANDO-HGT: Heterogeneous Graph Transformers for Smart Contract Vulnerability Detection",
        "topics": "Technical Papers",
        "abstract": "Smart contracts in blockchains have been increasingly used for high-value business applications. It is essential to check smart contracts\u2019 reliability before and after deployment. Although various program analysis and deep learning techniques have been proposed to detect vulnerabilities in either Ethereum smart contract source code or bytecode, their detection accuracy and generalizability are still limited. This paper presents a novel framework named MANDO-HGT for detecting smart contract vulnerabilities. Given Ethereum smart contracts, either in source code or bytecode form, either vulnerable or clean, MANDO-HGT custom-builds heterogeneous contract graphs (HCGs) to represent control-flow and/or function-call information of the code. It then adapts heterogeneous graph transformers (HGTs) with customized meta relations for graph nodes and edges to learn their embeddings and train classifiers for detecting various vulnerability types in the nodes and graphs of the contracts more accurately. We have collected more than 55K Ethereum smart contracts from various data sources and verified the labels for 423 buggy and 2,742 clean contracts to evaluate MANDO-HGT. Our empirical results show that MANDO-HGT can significantly improve the detection accuracy of other state-of-the-art vulnerability detection techniques that are based on whether machine learning or conventional analysis techniques. The improvements in accuracy in terms of F1-score range from 0.7% to more than 76% at either the coarse-grained contract level or the fine-grained line level for various vulnerability types in either source code or bytecode. Our method is general and can be retrained easily for different vulnerability types without the need for manually defined patterns for the vulnerabilities.",
        "authors": [
            {
                "name": "Hoang H. Nguyen",
                "institution": "L3S Research Center, Leibniz Universit\u00e4t Hannover",
                "country": "Germany"
            },
            {
                "name": "Nhat-Minh Nguyen",
                "institution": "Singapore Management University",
                "country": "Singapore"
            },
            {
                "name": "Chunyao Xie",
                "institution": "L3S Research Center, Leibniz Universit\u00e4t Hannover",
                "country": "Germany"
            },
            {
                "name": "Zahra Ahmadi",
                "institution": "L3S Research Center, Leibniz Universit\u00e4t Hannover",
                "country": "Germany"
            },
            {
                "name": "Daniel Kudenko",
                "institution": "L3S Research Center, Leibniz Universit\u00e4t Hannover",
                "country": "Germany"
            },
            {
                "name": "Thanh-Nam Doan",
                "institution": "Independent Researcher",
                "country": "USA"
            },
            {
                "name": "Lingxiao Jiang",
                "institution": "Singapore Management University",
                "country": "Singapore"
            }
        ]
    },
    {
        "title": "Towards Code-Aware AI Models for Code",
        "topics": "Keynotes",
        "abstract": "The past decade has seen unprecedented growth in Software Engineering\u2014 developers spend enormous time and effort to create new products. With such enormous growth comes the responsibility of producing and maintaining quality and robust software. In this talk, I will discuss how AI can help develop quality products in different stages of the software development life cycle. In particular, I will discuss how we can build AI models leveraging different static and dynamic code properties for source and binary code to automate diverse Software Engineering tasks, including code generation, bug finding, security analysis, etc.",
        "authors": [
            {
                "name": "Baishakhi RayKeynote Speaker",
                "institution": "Columbia University",
                "country": "United States"
            }
        ]
    },
    {
        "title": "Tutorial: Mining and Analysing Collaboration in git Repositories with git2net",
        "topics": "Tutorials",
        "abstract": "In this 40-min tutorial, attendees will learn how to mine fine-grained social networks from any git repository using git2net. Git repositories are used for a wide range of applications, such as the collaboration on scientific publications and joint collections of code snippets to fully-featured coding schools and popular software projects such as Linux or Firefox. The repositories link all changes made in a set of files to their corresponding authors. In addition, most git repositories, e.g., those of Open Source software projects, are freely accessible to researchers. Therefore, they represent rich and accessible sources of data on social interactions. With git2net, attendees will learn how to utilize git data for mining temporal interaction networks between authors of the repository.",
        "authors": [
            {
                "name": "Christoph Gote",
                "institution": "Chair of Systems Design, ETH Zurich",
                "country": "Switzerland"
            }
        ]
    },
    {
        "title": "An Empirical Study to Investigate Collaboration Among Developers in Open Source Software (OSS)",
        "topics": "Mining Challenge",
        "abstract": "Project owners are realizing the benefits of teamwork, leading to increased recognition of collaboration among developers in software engineering. A good understanding of how developers work together could positively impact software development practices. To investigate the collaboration habits of developers in project files we leverage the World of Code (WoC) dataset and GitHub API, in this paper. We first identify the collaborations levels of developers in the project files, such as the source, test, documentation, and build files, using the Author Cross Entropy (ACE). We find out that test files report the highest degree of collaboration among the developers, perhaps because collaboration is critical to ensure the convergence of functionality tests. Furthermore, the source code files show the least degree of collaboration, perhaps because of code ownership and the complexity and difficulty in code modification. Secondly, given the widespread usage of the Python programming language, we investigate the Python code tokens that are more prone to change and require collaboration. Our findings offer insights into the specific project files and Python code tokens that developers typically collaborate on in the open-source community. This information can be used by researchers and developers to enhance existing collaboration platforms and tools.",
        "authors": [
            {
                "name": "Weijie Sun",
                "institution": "University of Alberta",
                "country": "Canada"
            },
            {
                "name": "Samuel Iwuchukwu",
                "institution": "University of Alberta",
                "country": "Canada"
            },
            {
                "name": "Abdul Ali Bangash",
                "institution": "University of Alberta, Canada",
                "country": "Canada"
            },
            {
                "name": "Abram Hindle",
                "institution": "University of Alberta",
                "country": "Canada"
            }
        ]
    },
    {
        "title": "Keep the Ball Rolling: Analyzing Release Cadence in GitHub Projects",
        "topics": "Mining Challenge",
        "abstract": "Release cadence is the measure of time between software releases, both internal and external. Few studies analyze popular open-source projects\u2019 release cadence and use. In this work, we gathered over 8,000 GitHub projects from four popular programming languages; Go, Java, Python, and Ruby. Project were categorized intoslow,modern,rapid, andrapid+release cadence groups. We determined that only 13% of projects had a rapid release cadence of under 30 days. Applying NLP and topic modeling, we extracted the top 5 frequent topics for programming languages and obtained insights into their common uses. For example, Go projects are commonly used for Kubernetes tooling, while Ruby projects often leverage Rails for web development. We observed no significant relationship between frequent topics and the release cadence categories. This finding suggests release cadences are independent of the type of software delivered for a programming language. The replication package of our work is publicly available.",
        "authors": [
            {
                "name": "Oz Kilic",
                "institution": "Carleton University",
                "country": "Canada"
            },
            {
                "name": "Nathaniel Bowness",
                "institution": "University of Ottawa",
                "country": "Canada"
            },
            {
                "name": "Olga Baysal",
                "institution": "Carleton University",
                "country": "Canada"
            }
        ]
    },
    {
        "title": "Evolution of the Practice of Software Testing in Java Projects",
        "topics": "Mining Challenge",
        "abstract": "Software testing helps developers minimize bugs and errors in their code, improving the overall software quality. In 2013, Kochhar \\textit{et al.}~[1] analyzed 20,817 software projects in order to study how prevalent the practice of software testing is in open-source projects. They found that projects with more lines of code (LOC) and projects with more developers tend to have more test cases. Additionally, they found a weak positive correlation between the number of test cases and the number of bugs. Since the conclusions of a study might become irrelevant over time because of the latest practices in the relevant fields, in this paper, we investigate if these conclusions remain stable if we re-evaluate Kochhar \\textit{et al.}\u2019s findings on the Java projects that were developed from 2012 to 2021. For evaluation, we use a random sample of 20,000 open-source Java projects each year. Our results show that Kochhar \\textit{et al.}\u2019s conclusions regarding the LOC and the weak positive correlation between the number of test cases and bugs remain stable until 2021, and the conclusion regarding the correlation between the number of developers with respect to the number of test cases per developer varies. Our study corroborates most of Kochhar \\textit{et al.}\u2019s findings and uncovers the slight changes in their conclusions over the years to help developers refocus in light of the latest findings regarding the practice of software testing.",
        "authors": [
            {
                "name": "Anisha Islam",
                "institution": "Department of Computing Science, University of Alberta",
                "country": null
            },
            {
                "name": "Nipuni Tharushika Hewage",
                "institution": "Department of Computing Science, University of Alberta",
                "country": null
            },
            {
                "name": "Abdul Ali Bangash",
                "institution": "University of Alberta",
                "country": "Canada"
            },
            {
                "name": "Abram Hindle",
                "institution": "University of Alberta",
                "country": "Canada"
            }
        ]
    },
    {
        "title": "Do Subjectivity and Objectivity Always Agree? A Case Study with Stack Overflow Questions",
        "topics": "Technical Papers",
        "abstract": "In Stack Overflow (SO), the quality of posts (i.e., questions and answers) is subjectively evaluated by users through a voting mechanism. The net votes (upvotes - downvotes) obtained by a post are often considered an approximation of its quality. However, about half of the questions that received working solutions got more downvotes than upvotes. Furthermore, about 18% of the accepted answers (i.e., verified solutions) also do not score the maximum votes. All these counter-intuitive findings cast doubts on the reliability of the evaluation mechanism employed at SO. Moreover, many users raise concerns against the evaluation, especially downvotes to their posts. Therefore, rigorous verification of the subjective evaluation is highly warranted to ensure a non-biased and reliable quality assessment mechanism. In this article, we compare the subjective assessment of questions with their objective assessment using 2.5 million questions and ten text analysis metrics. According to our investigation, (1) four objective metrics agree, (2) two metrics do not agree, (3) one metric either agrees or disagrees, and (4) the remaining three metrics neither agree nor disagree with the subjective evaluation. We then develop machine learning models to classify the promoted and discouraged questions. Our models outperform the state-of-the-art models with a maximum of about 76%\u201387% accuracy.",
        "authors": [
            {
                "name": "Saikat Mondal",
                "institution": "University of Saskatchewan",
                "country": "Canada"
            },
            {
                "name": "Masud Rahman",
                "institution": "Dalhousie University",
                "country": "Canada"
            },
            {
                "name": "Chanchal K. Roy",
                "institution": "University of Saskatchewan",
                "country": "Canada"
            }
        ]
    },
    {
        "title": "The Secret Life of CVEs",
        "topics": "Mining Challenge",
        "abstract": "The Common Vulnerabilities and Exposures (CVEs) system is a reference method for documenting publicly known information security weaknesses and exposures. This paper presents a study of the lifetime of CVEs in software projects and the risk factors affecting their existence. The study uses survival analysis to examine how features of programming languages, projects and CVEs themselves impact the lifetime of CVEs. We suggest avenues for future research to investigate the effect of various factors on the resolution of vulnerabilities.",
        "authors": [
            {
                "name": "Piotr Przymus",
                "institution": "Nicolaus Copernicus University in Toru\u0144",
                "country": "Poland"
            },
            {
                "name": "Miko\u0142aj Fejzer",
                "institution": "Nicolaus Copernicus University in Toru\u0144",
                "country": "Poland"
            },
            {
                "name": "Jakub Nar\u0119bski",
                "institution": "Nicolaus Copernicus University in Toru\u0144",
                "country": "Poland"
            },
            {
                "name": "Krzysztof Stencel",
                "institution": "University of Warsaw",
                "country": "Poland"
            }
        ]
    },
    {
        "title": "Insights into Female Contributions in Open-Source Projects",
        "topics": "Mining Challenge",
        "abstract": "This paper presents a large quantitative study of the contributions of females compared to males in open-source projects. Female participation is found substantially low and females are found more engaged in non-coding work. The findings are statistically significant and are derived from an in-depth analysis of over 10 thousand developers\u2019 contributions to more than 81 million different projects in the World of Code (WoC) infrastructure. The insights from this study are useful in addressing gender disparity in the field.",
        "authors": [
            {
                "name": "Arifa Islam Champa",
                "institution": "Idaho State University",
                "country": "United States"
            },
            {
                "name": "Md Fazle Rabbi",
                "institution": "Idaho State University",
                "country": "United States"
            },
            {
                "name": "Minhaz F. Zibran",
                "institution": "Idaho State University",
                "country": "United States"
            },
            {
                "name": "Md Rakibul Islam",
                "institution": "University of Wisconsin - Eau Claire",
                "country": "United States"
            }
        ]
    },
    {
        "title": "Understanding the Role of Images on Stack Overflow",
        "topics": "Technical Papers",
        "abstract": "Images are increasingly being shared by software developers in diverse channels including question-and-answer forums like Stack Overflow. Although prior work has pointed out that these images are meaningful and provide complementary information compared to their associated text, how images are used to support questions is empirically unknown. To address this knowledge gap, in this paper we specifically conduct an empirical study to investigate (I) the characteristics of images, (II) the extent to which images are used in different question types, and (III) the role of images on receiving answers. Our results first show that user interface is the most common image content and undesired output is the most frequent purpose for sharing images. Moreover, these images essentially facilitate the understanding of 68% of sampled questions. Second, we find that discrepancy questions are more relatively frequent compared to those without images, but there are no significant differences observed in description length in all types of questions. Third, the quantitative results statistically validate that questions with images are more likely to receive accepted answers, but do not speed up the time to receive answers. Our work demonstrates the crucial role that images play by approaching the topic from a new angle and lays the foundation for future opportunities to use images to assist in tasks like generating questions and identifying question-relatedness.",
        "authors": [
            {
                "name": "Dong Wang",
                "institution": "Kyushu University",
                "country": "Japan"
            },
            {
                "name": "Tao Xiao",
                "institution": "Nara Institute of Science and Technology",
                "country": "Japan"
            },
            {
                "name": "Christoph Treude",
                "institution": "University of Melbourne",
                "country": "Australia"
            },
            {
                "name": "Raula Gaikovina Kula",
                "institution": "Nara Institute of Science and Technology",
                "country": "Japan"
            },
            {
                "name": "Hideaki Hata",
                "institution": "Shinshu University",
                "country": "Japan"
            },
            {
                "name": "Yasutaka Kamei",
                "institution": "Kyushu University",
                "country": "Japan"
            }
        ]
    },
    {
        "title": "A Dataset of Bot and Human Activities in GitHub",
        "topics": "Data and Tool Showcase Track",
        "abstract": "Software repositories hosted on GitHub frequently use development bots to automate repetitive, effort-intensive and error-prone tasks. To understand and study how these bots are used, state-of-the-art bot identification tools have been developed to detect bots based on their comments in commits, issues and pull requests. Given that bots can be involved in many other activity types, there is a need to consider more activities that they are carrying out in the software repositories they are involved in. We, therefore, propose a curated dataset of such activities carried out by bots and humans involved in GitHub repositories. The dataset was constructed by identifying 24 high-level activity types that could be extracted from 15 lower-level GitHub event types that were queried from GitHub\u2019s event stream API for all considered bots and humans. The proposed dataset contains around 600K activities performed by 384 bots and 585 humans involved in GitHub repositories, during an observation period ranging from 25 November 2022 to 25 January 2023. This dataset is valuable for future empirical studies focusing on how bots impact the software development process.",
        "authors": [
            {
                "name": "Natarajan Chidambaram",
                "institution": "University of Mons",
                "country": "Belgium"
            },
            {
                "name": "Alexandre Decan",
                "institution": "University of Mons; F.R.S.-FNRS",
                "country": "Belgium"
            },
            {
                "name": "Tom Mens",
                "institution": "University of Mons",
                "country": "Belgium"
            }
        ]
    },
    {
        "title": "What Warnings Do Engineers Really Fix? The Compiler That Cried Wolf",
        "topics": "Industry Track",
        "abstract": "Build logs from a variety of Continuous Integration (CI) systems contain temporal data about the presence and distribution of compiler warnings. Results from the analysis and mining of that data will indicate what warnings engineers find useful and fix, or continuously ignore. The findings will include resolution times and resolution types for different warning categories. That data will help compiler developers adjust the warning levels according to the ground truth, clarify the diagnostic messages, and improve the non-actionable warnings. The empirical findings will also help engineers to decide what warnings are worth fixing and which ones are not.",
        "authors": [
            {
                "name": "Gunnar Kudrjavets",
                "institution": "University of Groningen",
                "country": "United States"
            },
            {
                "name": "Aditya Kumar",
                "institution": "Snap, Inc.",
                "country": null
            },
            {
                "name": "Ayushi Rastogi",
                "institution": "University of Groningen, The Netherlands",
                "country": "Netherlands"
            }
        ]
    },
    {
        "title": "PENTACET data - 23 Million Code Comments and 500,000 SATD comments",
        "topics": "Data and Tool Showcase Track",
        "abstract": "Most SATD research utilizes non-probabilistic sampling for data selection, which weakens the empirical findings\u2019 generalization capability. A closer look reveals several SATD research are based on simple (`Easy to find\u2019) code comments without the contextual data (preceding and succeeding source code context). In this work, we address this gap through PENTACET (or 5C) dataset. PENTACET is a large Curated Contextual Code Comments per Contributor and the most extensive SATD data. It is acquired by mining 9,096 Open Source Software Java projects with a total of 435 million LOC and captures bi-directional contextual information of all source code granularities in more than 26 million source code files. The outcome is data set with 23 million code comments, source code context for each comment, and more than 500,000 comments labeled as SATD.",
        "authors": [
            {
                "name": "Murali Sridharan",
                "institution": "University of Oulu",
                "country": "Finland"
            },
            {
                "name": "Leevi Rantala",
                "institution": "University of Oulu",
                "country": "Finland"
            },
            {
                "name": "Mika M\u00e4ntyl\u00e4",
                "institution": "University of Oulu",
                "country": "Finland"
            }
        ]
    },
    {
        "title": "DocMine: A Software Documentation-Related Dataset of 950 GitHub Repositories",
        "topics": "Data and Tool Showcase Track",
        "abstract": "Software documentation is one of the critical aspects of a software project, that could support multiple tasks throughout the software development life-cycle. There is extensive research on understanding issues and challenges with existing documentation, which is typically available as readme files. In projects that support collaborative development, such as those on GitHub, other software artifacts such as commits, pull requests and issues, apart from the conventional readme files, wikis and source code comments,  also contain useful information, that supports in understanding, using, extending and maintaining the project. However, we are not aware of any dataset that explicitly focuses on documentation-related information in multiple software artifacts such as readme files, commits and pull requests across a repository. To address this concern and to facilitate further research in software documentation, we present DocMine, as a dataset of documentation-related information, extracted from around 1.35M software artifacts in 950 GitHub repositories, spanning across four different programming languages. The dataset along with its documentation is made available in CSV and .sql formats at -https://doi.org/10.5281/zenodo.5195084.",
        "authors": [
            {
                "name": "Akhila Sri Manasa Venigalla",
                "institution": "IIT Tirupati",
                "country": "India"
            },
            {
                "name": "Sridhar Chimalakonda",
                "institution": "IIT Tirupati",
                "country": "India"
            }
        ]
    },
    {
        "title": "GiveMeLabeledIssues: An Open Source Issue Recommendation System",
        "topics": "Data and Tool Showcase Track",
        "abstract": "Developers often struggle to identify the skills required to work on open issues in Open Source Software (OSS) projects. Proper issue labeling can help task selection, but current strategies are limited to classifying the issues according to their type (e.g., bug, question, good first issue, feature, etc.). In contrast, this paper presents a tool that mines project repositories and labels issues based on the skills required to solve them, more specifically the domain of the APIs involved in the solution (e.g., User Interface (UI), Test, Databases (DB), etc.). GiveMeLabeledIssues facilitates matching developers\u2019 skills and tasks, reducing the burden on project maintainers by minimizing the amount of manual labeling needed to annotate project issues effectively. The demo toll obtained a precision of 83.9% predicting projects with TF-IDF and Random Forest (RF).",
        "authors": [
            {
                "name": "Joseph Vargovich",
                "institution": "Northern Arizona University",
                "country": null
            },
            {
                "name": "Fabio Marcos De Abreu Santos",
                "institution": "Northern Arizona University",
                "country": "USA"
            },
            {
                "name": "Jacob Penney",
                "institution": "Northern Arizona University",
                "country": null
            },
            {
                "name": "Marco Gerosa",
                "institution": "Northern Arizona University",
                "country": null
            },
            {
                "name": "Igor Steinmacher",
                "institution": "Northern Arizona University",
                "country": null
            }
        ]
    },
    {
        "title": "DACOS-A Manually Annotated Dataset of Code Smells",
        "topics": "Data and Tool Showcase Track",
        "abstract": "Researchers apply machine-learning techniques for code smell detection to counter the subjectivity of many code smells. Such approaches need a large, manually annotated  dataset for training and benchmarking. Existing literature offers a few datasets; however, they are small in size and, more importantly, do not focus on the subjective code snippets. In this paper, we presentDACOS, a manually annotated dataset containing 10,267 annotations for 5,192 code snippets. The dataset targets three kinds of code smells at different granularity-multifaceted abstraction, complex method, andlong parameter list. The dataset is created in two phases. The first phase helps us identify the code snippets that are potentially subjective by determining the thresholds of metrics used to detect a smell. The second phase collects annotations for potentially subjective snippets. We also offer an extended datasetDACOSXthat includes definitely benign and definitely smelly snippets by using the thresholds identified in the first phase. We have developedTagman, a web application to help annotators view and mark the snippets one-by-one and record the provided annotations. We make the datasets and the web application accessible publicly. This dataset will help researchers working on smell detection techniques to build relevant and context-aware machine-learning models.",
        "authors": [
            {
                "name": "Himesh Nandani",
                "institution": "Dalhousie University",
                "country": "Canada"
            },
            {
                "name": "Mootez Saad",
                "institution": "Dalhousie University",
                "country": "Canada"
            },
            {
                "name": "Tushar Sharma",
                "institution": "Dalhousie University",
                "country": "Canada"
            }
        ]
    },
    {
        "title": "A Large Scale Analysis of Semantic Versioning in NPM",
        "topics": "Technical Papers",
        "abstract": "The NPM package repository contains over two million packages and serves tens of billions of downloads per-week. Nearly every single JavaScript application uses the NPM package manager to install packages from the NPM repository. NPM relies on a \u201csemantic versioning\u201d (\u2018semver\u2019) scheme to maintain a healthy ecosystem, where bug-fixes are reliably delivered to downstream packages as quickly as possible, while breaking changes require manual intervention by downstream package maintainers. In order to understand how developers use semver, we build a dataset containing every version of every package on NPM and analyze the flow of updates throughout the ecosystem. We build a time-travelling dependency resolver for NPM, which allows us to determine precisely which versions of each dependency would have been resolved at different times. We segment our analysis to allow for a direct analysis of security-relevant updates (those that introduce or patch vulnerabilities) in comparison to the rest of the ecosystem. We find that when developers use semver correctly, critical updates such as security patches can flow quite rapidly to downstream dependencies in the majority of cases (90.09%), but this does not always occur, due to developers\u2019 imperfect use of both semver version constraints and semver version number increments. Our findings have implications for developers and researchers alike. We make our infrastructure and dataset publicly available under an open source license.",
        "authors": [
            {
                "name": "Donald Pinckney",
                "institution": "Northeastern University",
                "country": "United States"
            },
            {
                "name": "Federico Cassano",
                "institution": "Northeastern University",
                "country": "United States"
            },
            {
                "name": "Arjun Guha",
                "institution": "Northeastern University and Roblox Research",
                "country": "United States"
            },
            {
                "name": "Jonathan Bell",
                "institution": "Northeastern University",
                "country": "United States"
            }
        ]
    },
    {
        "title": "Automating Arduino Programming: From Hardware Setups to Sample Source Code Generation",
        "topics": "Technical Papers",
        "abstract": "An embedded system is a system consisting of software code, controller hardware, and Input/Output hardware that performs a specific task. There are several challenges when developing an embedded system. First, the code often involves hardware configurations that require domain-specific knowledge. Second, the hardware may have code usage patterns that should be followed. To overcome such challenges, we propose a framework called ArduinoProg towards automatic generation of Arduino applications. ArduinoProg takes natural language queries as input, then outputs hardware configurations and code usage patterns of the hardware for the query. Motivated by our findings on the characteristics of real-world queries posted in the official Arduino forum, we formulate ArduinoProg as three components, i.e., Library Retriever, Hardware Classifier, and API Generator. First, Library Retriever preprocesses the input query and retrieves a set of relevant library names using either lexical matching or vector-based similarity. Second, given Library Retriever\u2019s output, Hardware Classifier infers the hardware configuration by classifying the method definitions from the implementation files of a library into certain communication protocol classes. Third, API Generator leverages a sequence-to-sequence model to generate the code usage patterns also based on the Library Retriever\u2019s output. Having instantiated each component of ArduinoProg  with various machine learning models, we have evaluated ArduinoProg on real-world queries. The performance of Library Retriever ranges from 44.0%-97.1% in terms of Precision@K; the Hardware Classifier classifier can achieve 0.79-0.92 in terms of the area under the Receiver Operating Characteristics curve (AUC); API Generator can yield 0.45-0.73 in terms of Normalized Discounted Cumulative Gain (NDCG)@K. Demo:https://youtu.be/d8E4Zjrs_KQ",
        "authors": [
            {
                "name": "Imam Nur Bani Yusuf",
                "institution": "Singapore Management University",
                "country": "Indonesia"
            },
            {
                "name": "Diyanah Binte Abdul Jamal",
                "institution": "Singapore Management University",
                "country": null
            },
            {
                "name": "Lingxiao Jiang",
                "institution": "Singapore Management University",
                "country": "Singapore"
            }
        ]
    },
    {
        "title": "Boosting Just-in-Time Defect Prediction with Specific Features of C Programming Languages in Code Changes",
        "topics": "Technical Papers",
        "abstract": "Just-in-time (JIT) defect prediction can identify changes as defect-inducing ones or clean ones and many approaches are proposed based on several programming language-independent change-level features. However, different programming languages have different characteristics and consequently may affect the quality of software projects. Meanwhile, the C programming language, one of the most popular ones, is widely used to develop foundation applications (i.e., operating system, database, compiler, etc.) in IT companies and its change-level characteristics on project quality have not been fully investigated. Additionally, whether open-source C projects have similar important features to commercial projects has not been studied much.",
        "authors": [
            {
                "name": "Chao Ni",
                "institution": "Zhejiang University",
                "country": "China"
            },
            {
                "name": "xiaodanxu",
                "institution": "College of Computer Science and Technology, Zhejiang university",
                "country": null
            },
            {
                "name": "Kaiwen Yang",
                "institution": "Zhejiang University",
                "country": "China"
            },
            {
                "name": "David Lo",
                "institution": "Singapore Management University",
                "country": "Singapore"
            }
        ]
    },
    {
        "title": "Pre-trained Model Based Feature Envy Detection",
        "topics": "Technical Papers",
        "abstract": "Code smell slows down software system development and makes them harder to maintain. Existing research aims to develop automatic detection algorithms to reduce the labor and time costs within the detection process. Deep learning techniques have recently been demonstrated to enhance the performance of recognizing code smell even more than metric-based heuristic detection algorithms. As Large-scale pre-trained models for Programming Languages (PL), such as CodeT5, have lately achieved the top results in a variety of downstream tasks, some researchers begin to explore the use of pre-trained models to extract the contextual semantics of code to detect code smells. However, little research has employed contextual code semantics relationship between code snippets obtained by pre-trained models to identify code smells. In this paper, we investigate the use of the pre-trained model codeT5 to extract semantic relationships between code snippets to detect feature envy, which is one of the most common code smells. In addition, to investigate the performance of these semantic relationships extracted by pre-trained models of different architectures on detecting feature envy, we compare CodeT5 with two other pre-trained models CodeBert, CodeGPT. % We have performed our experimental evaluation on ten open-source projects, our approach outperforms the state-of-the-art in F-measure with a 29.32% improvement on detecting feature envy and in accuracy with a 16.57% improvement on moving destination recommendation. We have performed our experimental evaluation on ten open-source projects, our approach improves F-measure by 29.32% on feature envy detection and 16.57% on moving destination recommendation. And using semantic relations extracted by several pre-trained models to detect feature envy outperforms the state-of-the-art. This shows that using this semantic relation to detect feature envy is promising. To enable future research on feature envy detection, we have made all the code and datasets utilized in this article open source.",
        "authors": [
            {
                "name": "mawenhao",
                "institution": "Wuhan University",
                "country": "China"
            },
            {
                "name": "Yaoxiang Yu",
                "institution": "Wuhan University",
                "country": "China"
            },
            {
                "name": "Xiaoming Ruan",
                "institution": "Wuhan University",
                "country": "China"
            },
            {
                "name": "Bo Cai",
                "institution": "Wuhan University",
                "country": "China"
            }
        ]
    },
    {
        "title": "CLEAN++: Code Smells Extraction for C++",
        "topics": "Data and Tool Showcase Track",
        "abstract": "The extraction of features is an essential step in the process of mining software repositories. An important feature that has been actively studied in the field of mining software repositories is bad code smells. Bad code smells are patterns in the source code that indicate an underlying issue in the design and implementation of the software. Several tools have been proposed to extract code smells. However, currently there are no tools that extract a significant number of code smells from software written in C++. Therefore, we propose CLEAN++ (Code smeLls ExtrActioN for c++). It is an extension of a robust static code analysis tool that implements 35 code smells. To evaluate CLEAN++, we ran it over 44 open-source projects and wrote test cases to validate each code smell. Also, we converted the test cases to Java and used two Java tools to validate the effectiveness of our tool. In the end, we confirmed that the CLEAN++ is successful at detecting code smells. The tool is available athttps://github.com/Tomma94/CLEAN-Plus-Plus.",
        "authors": [
            {
                "name": "Tom Mashiach",
                "institution": "Ben Gurion University of the Negev, Israel",
                "country": "Israel"
            },
            {
                "name": "Bruno Sotto-Mayor",
                "institution": "Ben Gurion University of the Negev, Israel",
                "country": "Israel"
            },
            {
                "name": "Gal Kaminka",
                "institution": "Bar Ilan University, Israel",
                "country": "Israel"
            },
            {
                "name": "Meir Kalech",
                "institution": "Ben Gurion University of the Negev, Israel",
                "country": "Israel"
            }
        ]
    },
    {
        "title": "Mining the Characteristics of Jupyter Notebooks in Data Science Projects",
        "topics": "Registered Reports",
        "abstract": "Nowadays, numerous industries have exceptional demand for skills in data science, such as data analysis, data mining, and machine learning. The computational notebook (e.g., Jupyter notebook) is a well-known data science tool adopted in practice. Kaggle and GitHub are two platforms where data science communities are used for knowledge-sharing,  skill-practicing, and collaboration. While tutorials and guidelines for novice data science are available on both platforms, there is a low number of Jupyter notebooks that received high numbers of votes from the community. The high-voted notebook is considered well-documented, easy to understand, and applies the best data science and software engineering practices. In this research, we aim to understand the characteristics of high-voted Jupyter notebooks on Kaggle and the popular Jupyter notebooks for data science projects on GitHub. We plan to mine and analyze the Jupyter notebooks on both platforms. We will perform exploratory analytics, data visualization, and feature importances to understand the overall structure of these notebooks and to identify common patterns and best-practice features separating the low-voted and high-voted notebooks. Upon the completion of this research, the discovered insights can be applied as training guidelines for aspiring data scientists and machine learning practitioners looking to improve their performance from novice ranking Jupyter notebook on Kaggle to a deployable project on GitHub.",
        "authors": [
            {
                "name": "Morakot Choetkiertikul",
                "institution": "Mahidol University",
                "country": "Thailand"
            },
            {
                "name": "Apirak Hoonlor",
                "institution": "Mahidol University",
                "country": "Thailand"
            },
            {
                "name": "Chaiyong Ragkhitwetsagul",
                "institution": "Mahidol University",
                "country": "Thailand"
            },
            {
                "name": "Siripen Pongpaichet",
                "institution": "Mahidol University",
                "country": "Thailand"
            },
            {
                "name": "Thanwadee Sunetnanta",
                "institution": "Mahidol University",
                "country": "Thailand"
            },
            {
                "name": "Tasha Settewong",
                "institution": "Mahidol University",
                "country": "Thailand"
            },
            {
                "name": "Raula Gaikovina Kula",
                "institution": "Nara Institute of Science and Technology",
                "country": "Japan"
            }
        ]
    },
    {
        "title": "Optimizing Duplicate Size Thresholds in IDEs",
        "topics": "Industry Track",
        "abstract": "In this paper, we present an approach for transferring an optimal lower size threshold for clone detection from one language to another by analyzing their clone distributions. We showcase this method by transferring the threshold from regular Python scripts to Jupyter notebooks for using in two JetBrains IDEs, Datalore and DataSpell.",
        "authors": [
            {
                "name": "Konstantin Grotov",
                "institution": "JetBrains Research, Constructor University",
                "country": null
            },
            {
                "name": "Sergey Titov",
                "institution": "JetBrains Research",
                "country": null
            },
            {
                "name": "Alexandr Suhinin",
                "institution": "JetBrains",
                "country": null
            },
            {
                "name": "Yaroslav Golubev",
                "institution": "JetBrains Research",
                "country": "Serbia"
            },
            {
                "name": "Timofey Bryksin",
                "institution": "JetBrains Research",
                "country": "Cyprus"
            }
        ]
    },
    {
        "title": "Don't Forget the Exception! Considering Robustness Changes to Identify Design Problems",
        "topics": "Technical Papers",
        "abstract": "Modern programming languages, such as Java, use exception-handling mechanisms to guarantee the robustness of software systems. Although important, the quality of exception code is usually poor and neglected by developers. Indiscriminate robustness changes (e.g., the addition of empty catch blocks) can indicate design decisions that negatively impact the internal quality of software systems. As it is known in the literature, multiple occurrences of poor code structures, namely code smells, are strong indicators of design problems. Still, existing studies focus mainly on the correlation of maintainability smells with design problems. However, using only these smells may not be enough since developers need more context (e.g., system domain) to identify the problems in certain scenarios. Moreover, these studies do not explore how changes in the exceptional code of the methods combined with maintainability smells can give complementary evidence of design problems. By covering both regular and exception codes, the developer can have more context about the system and find complementary code smells that reinforce the presence of design problems. This work aims to leverage the identification of design problems by tracking poor robustness changes combined with maintainability smells. We investigated the correlation between robustness changes and maintainability smells on the commit history of more than 160k methods from different releases of 10 open-source software systems. We observed that maintainability smells can be worsened or even introduced when robustness changes are performed. This scenario mainly happened for the Feature Envy, Long Method, and Dispersed Coupling smells. We also analyzed the co-occurrence between robustness and maintainability smells. We identified that the empty catch block and catch throwable robustness smells were the ones that co-occurred the most with maintainability smells related to the Concern Overload and Misplaced Concern design problems. The contribution of our work is to reveal that poor exception code, usually neglected by developers, negatively impacts the overall quality of software systems. Therefore, existing code smell detecting tools can be enhanced to leverage robustness changes to identify design problems.",
        "authors": [
            {
                "name": "Anderson Oliveira",
                "institution": "PUC-Rio",
                "country": "Brazil"
            },
            {
                "name": "Jo\u00e3o Lucas Correia",
                "institution": "Federal University of Alagoas",
                "country": "Brazil"
            },
            {
                "name": "Leonardo Da Silva Sousa",
                "institution": "Carnegie Mellon University, USA",
                "country": "USA"
            },
            {
                "name": "Wesley Assun\u00e7\u00e3o",
                "institution": "Johannes Kepler University Linz, Austria & Pontifical Catholic University of Rio de Janeiro, Brazil",
                "country": "Austria"
            },
            {
                "name": "Daniel Coutinho",
                "institution": "PUC-Rio",
                "country": null
            },
            {
                "name": "Alessandro Garcia",
                "institution": "PUC-Rio",
                "country": "Brazil"
            },
            {
                "name": "Willian Oizumi",
                "institution": "GoTo",
                "country": "Brazil"
            },
            {
                "name": "Caio Barbosa",
                "institution": "UFAL",
                "country": null
            },
            {
                "name": "Anderson Uch\u00f4a",
                "institution": "Federal University of Cear\u00e1",
                "country": "Brazil"
            },
            {
                "name": "Juliana Alves Pereira",
                "institution": "PUC-Rio",
                "country": "Brazil"
            }
        ]
    },
    {
        "title": "Determining Open Source Project Boundaries",
        "topics": "Industry Track",
        "abstract": "This talk proposal discusses the challenge of determining the boundaries of an open source software project. While open source ecosystems have fluid membership by nature, explicit boundaries are necessary to conduct research and analysis around projects and their communities as these exercises must select a set number of repositories, forums, mailing lists, etc to count as part of this effort. The ideal solution to this problem would provide researchers and analysts with a common approach to identify what is part of or affiliated with a project community and ecosystem.",
        "authors": [
            {
                "name": "Sophia Vargas",
                "institution": "Google",
                "country": null
            }
        ]
    },
    {
        "title": "Tutorial: Beyond the leading edge. What else is out there?",
        "topics": "Tutorials",
        "abstract": "Predictions are often wrong, especially about the future. But if we were to take a bet on the future, what technologies might we see? LLM, deep learning, sure, but what ELSE is out there? This talk will focus on three leading edge and three bleeding edge trends:",
        "authors": [
            {
                "name": "Tim Menzies",
                "institution": "North Carolina State University",
                "country": "United States"
            }
        ]
    },
    {
        "title": "PyMigBench: A Benchmark for Python Library Migration",
        "topics": "Data and Tool Showcase Track",
        "abstract": "Developers heavily rely on Application Programming Interfaces (APIs) from libraries to build their projects. However, libraries might become obsolete, or new libraries with better APIs might become available. In such cases, developers replace the used libraries with alternative libraries, a process known as library migration. Since manually migrating between libraries is tedious and error prone, there has been a lot of effort towards automated library migration. However, most of the current research on automated library migration focuses on Java libraries, and even more so on version migrations of the same library. Despite the increasing popularity of Python, limited research has investigated migration between Python libraries. To provide the necessary data for advancing the development of Python library migration tools, this paper contributes PyMigBench, a benchmark of real Python library migrations. PyMigBench contains 59 analogous library pairs and 75 real migrations with migration-related code changes in 160 Python files across 57 client repositories.",
        "authors": [
            {
                "name": "Mohayeminul Islam",
                "institution": "University of Alberta",
                "country": "Canada"
            },
            {
                "name": "Ajay Jha",
                "institution": "North Dakota State University",
                "country": "United States"
            },
            {
                "name": "Sarah Nadi",
                "institution": "University of Alberta",
                "country": "Canada"
            },
            {
                "name": "Ildar Akhmetov",
                "institution": "University of Alberta",
                "country": "Canada"
            }
        ]
    },
    {
        "title": "Phylogenetic Analysis of Reticulate Software Evolution",
        "topics": "Technical Papers",
        "abstract": "In this paper, we apply techniques from phylogenetics for uncovering evolutionary dependencies among software versions. Phylogenetics is a part of computational molecular biology that addresses the in ference of evolution among organisms based on differences/similarities in DNA sequences and morphology. We apply a tree differencing technique to abstract syntax trees to calculate a distance matrix, which is then used by a distance-based phylogenetic algorithm to infer an evolution network. This allows us to identify merging and branching among versions without manually looking into the details of the source code. Experiments on ancient/old versions of the Emacs editor and the open source 3D printer firmware show that we not only can reproduce the evolution of the software but also can identify code import/merging across different lineages. We also discuss how the techniques are used to identify the feature models among software variations. To the best of our knowledge, this paper is the first to report on a reticulate phylogenetic analysis of the software and may offer a useful method for gaining information on the evolution of the software.",
        "authors": [
            {
                "name": "Akira Mori",
                "institution": "National Institute of Advanced Industrial Science and Technology",
                "country": "Japan"
            },
            {
                "name": "Masatomo Hashimoto",
                "institution": "Chiba Institute of Technology",
                "country": "Japan"
            }
        ]
    },
    {
        "title": "Method Chaining Redux: An Empirical Study of Method Chaining in Java, Kotlin, and Python",
        "topics": "Technical Papers",
        "abstract": "There are possible benefits and drawbacks to chaining methods together, as is often done in fluent APIs. A prior study investigated how Java developers chain methods in over 2.7k open-source projects. That study observed, for the dataset analyzed, that the use of method chaining in Java is popular and seems to be increasing over time. That study however was limited to a smaller sample of Java projects, and it is also not clear if the results generalize to other languages. In this work, we first replicate the prior results by building a similar dataset and our own analysis scripts. We then extend those results by analyzing a much larger dataset of 89k Java projects and generalizing to other programming languages by analyzing 26k Kotlin projects and 98k Python projects. The results show chaining is more popular in Java and Kotlin than Python, chaining use in Kotlin is not growing, and Python sees more use in non-testing code.",
        "authors": [
            {
                "name": "Ali Keshk",
                "institution": "University of Nebraska-Lincoln",
                "country": "United States"
            },
            {
                "name": "Robert Dyer",
                "institution": "University of Nebraska-Lincoln",
                "country": "United States"
            }
        ]
    },
    {
        "title": "Intertwining Communities: Exploring Libraries that Cross Software Ecosystems",
        "topics": "Technical Papers",
        "abstract": "Using libraries in applications have helped developers to reduce the costs of reinventing already existing code. However, an increase in diverse technology stacks and third-party library usage has led developers to inevitably switch technologies, and also search for similar libraries implemented in the new technology. To assist with searching for these replacement libraries, maintainers have started to release their libraries to multiple ecosystems. Our goal is to explore the extent to which these libraries are intertwined between ecosystems. We perform a large-scale empirical study of 1.1 million libraries from five different software ecosystems, i.e., PyPI, CRAN, Maven, RubyGems, and NPM, to identify 4,146 GitHub repositories. As a starting point, insights from the study raise implications for library maintainers, users, contributors, and researchers into understanding how these different ecosystems are becoming more intertwined with each other.",
        "authors": [
            {
                "name": "Kanchanok Kannee",
                "institution": "Nara Institute of Science and Technology",
                "country": "Japan"
            },
            {
                "name": "Raula Gaikovina Kula",
                "institution": "Nara Institute of Science and Technology",
                "country": "Japan"
            },
            {
                "name": "Supatsara Wattanakriengkrai",
                "institution": "Nara Institute of Science and Technology",
                "country": "Japan"
            },
            {
                "name": "Kenichi Matsumoto",
                "institution": "Nara Institute of Science and Technology",
                "country": "Japan"
            }
        ]
    },
    {
        "title": "Open Source Software Digital Sociology: Quantifying and Understanding Large Complex Open Source Ecosystems",
        "topics": "Vision and Reflection",
        "abstract": "Open Source Software (OSS) ecosystems have had a tremendous impact on computing and society, while their formation and sustainability pose great challenges to both practitioners and researchers. We utilize vast collections of open data produced by distributed version control and social media to discover the mechanisms by which such large complex ecosystems form and operate, which we call OSS digital sociology. We target critical issues ranging from individual learning, group collaboration, to ecosystem sustainability, and software supply chain. We discuss the preliminary promising results and their relevance in practice.",
        "authors": [
            {
                "name": "Minghui Zhou",
                "institution": "Peking University",
                "country": "China"
            }
        ]
    },
    {
        "title": "Large Language Models and Simple, Stupid Bugs",
        "topics": "Technical Papers",
        "abstract": "With the advent of powerful neural language models, AI-based systems to assist developers in coding tasks are becoming widely available; Copilot is one such system. Copilot uses Codex, a large language model (LLM), to complete code conditioned on a preceding \u201cprompt\u201d. Codex, however, is trained on public GitHub repositories, viz., on code that may include bugs and vulnerabilities. Previous studies [1], [2] show Codex reproduces vulnerabilities seen in training. In this study, we examine how prone Codex is to generate an interesting bug category, single statement bugs, commonly referred to as simple, stupid bugs or SStuBs in the MSR community. We find that Codex and similar LLMs do help avoid some SStuBs, but do produce known, verbatim SStuBs as much as 2x as likely than known, verbatim correct code. We explore the consequences of the Codex generated SStuBs and propose avoidance strategies that suggest the possibility of reducing the production of known, verbatim SStubs, and increase the possibility of producing known, verbatim fixes.",
        "authors": [
            {
                "name": "Kevin Jesse",
                "institution": "University of California at Davis",
                "country": "United States"
            },
            {
                "name": "Toufique Ahmed",
                "institution": "University of California at Davis",
                "country": "United States"
            },
            {
                "name": "Prem Devanbu",
                "institution": "University of California at Davis",
                "country": "United States"
            },
            {
                "name": "Emily Morgan",
                "institution": "University of California, Davis",
                "country": null
            }
        ]
    },
    {
        "title": "Snapshot Testing Dataset",
        "topics": "Data and Tool Showcase Track",
        "abstract": "Snapshot testing is a form of software testing that is focused on visual components by highlighting any code changes when compared to a previously stored state. This quick and simple method of testing is growing popular among the industry with companies such as Spotify and Robinhood. Despite its growing popularity, snapshot testing is barely explored in academia. In this paper, we use GitHub API to collect a dataset of 686 repositories tagged with Jest, a popular testing framework capable of snapshot testing. From those repositories, we found 4,604 snapshot files and 11,367 test files. The top-10 repositories represent 20% of all snapshot files in the dataset, even though it is only 3% of the size. We acknowledge that improvements can be made in the dataset but due to the lack of data on snapshot testing, we believe the current dataset is useful in helping researchers to study this topic.",
        "authors": [
            {
                "name": "Emily Bui",
                "institution": "Loyola University Maryland",
                "country": "United States"
            },
            {
                "name": "Henrique Rocha",
                "institution": "Loyola University Maryland, USA",
                "country": "Brazil"
            }
        ]
    },
    {
        "title": "Human-Centered AI for SE: Reflection and Vision",
        "topics": "Vision and Reflection",
        "abstract": "Since its inception in the 2000s, AI for Software Engineering (AI4SE) has grown rapidly, with the MSR community playing a pivotal role. By analyzing data in various repositories, AI in its different forms, e.g., data mining, information retrieval, machine learning, natural language processing, etc., has been demonstrated to be able to produce good results for automating many tasks, including specification mining, bug and vulnerability discovery, bug localization, duplicate bug report identification, failure detection, program repair, technical question answering, code search, and many more. AI4SE has much potential to improve software engineers\u2019 productivity and software quality. Due to its potential, it is currently one of the most popular research areas in the software engineering field.",
        "authors": [
            {
                "name": "David Lo",
                "institution": "Singapore Management University",
                "country": "Singapore"
            }
        ]
    },
    {
        "title": "Control and Data Flow in Security Smell Detection for Infrastructure as Code: Is It Worth the Effort?",
        "topics": "Technical Papers",
        "abstract": "Infrastructure as Code is the practice of developing and maintaining computing infrastructure through executable source code. Unfortunately, IaC has also brought about new cyber attack vectors. Prior work has therefore proposed static analyses that detect security smells in Infrastructure as Code files. However, they have so far remained at a shallow level, disregarding the control and data flow of the scripts under analysis, and may lack awareness of specific syntactic constructs. These limitations inhibit the quality of their results. To address these limitations, in this paper, we present GASEL, a novel security smell detector for the Ansible IaC language. It uses graph queries on program dependence graphs to detect 7 security smells. Our evaluation on an oracle of 243 real-world security smells and comparison against two state-of-the-art security smell detectors shows that awareness of syntax, control flow, and data flow enables our approach to substantially improve both precision and recall. We further question whether the additional effort required to develop and run such an approach is justified in practice. To this end, we investigate the prevalence of indirection through control and data flow in security smells across more than 15 000 Ansible scripts. We find that over 55% of security smells contain data-flow indirection, and over 32% require a whole-project analysis to detect. These findings motivate the need for deeper static analysis tools to detect security vulnerabilities in IaC.",
        "authors": [
            {
                "name": "Ruben Opdebeeck",
                "institution": "Vrije Universiteit Brussel",
                "country": "Belgium"
            },
            {
                "name": "Ahmed Zerouali",
                "institution": "Vrije Universiteit Brussel",
                "country": "Belgium"
            },
            {
                "name": "Coen De Roover",
                "institution": "Vrije Universiteit Brussel",
                "country": "Belgium"
            }
        ]
    },
    {
        "title": "Helm Charts for Kubernetes Applications: Evolution, Outdatedness and Security Risks",
        "topics": "Technical Papers",
        "abstract": "Using Kubernetes for the deployment, management and scaling of containerized applications has become a common practice. To facilitate the installation and management of these applications on Kubernetes clusters, practitioners can use the Helm package manager. Helm enables defining, installing and upgrade complex Kubernetes applications in an easy and organized way through Charts. Our goal is to support chart developers and users by assessing the state and evolution of publicly available charts, as well as the outdatedness and security risks of their images. For 9,482 charts that are distributed via the Artifact Hub repository, we mine and collect the list of their metadata, versions, dependencies, maintainers and container images. Then, we carry out an empirical analysis into seven aspects. We found that the ecosystem forming around Helm Charts ecosystem is growing fast. However, most of the Charts are not official with no popularity and no license. We also observed that charts tend to release multiple versions, but around half of them are still in the initial development phase. When looking at the container images used in charts, we found that around half of them are outdated and 88.1% of them are exposed to vulnerabilities, jeopardizing 93.7% of the charts.",
        "authors": [
            {
                "name": "Ahmed Zerouali",
                "institution": "Vrije Universiteit Brussel",
                "country": "Belgium"
            },
            {
                "name": "Ruben Opdebeeck",
                "institution": "Vrije Universiteit Brussel",
                "country": "Belgium"
            },
            {
                "name": "Coen De Roover",
                "institution": "Vrije Universiteit Brussel",
                "country": "Belgium"
            }
        ]
    },
    {
        "title": "A Study of Gender Discussions in Mobile Apps",
        "topics": "Technical Papers",
        "abstract": "Mobile software apps (\u201capps\u201d) are one of the prevailing digital technologies that our modern life heavily depends on. A key issue in the development of apps is how to design gender-inclusive apps. Apps that do not consider gender inclusion, diversity, and equality in their design can create barriers (e.g., excluding some of the users because of their gender) for their diverse users. While there have been some efforts to develop gender-inclusive apps, a lack of deep understanding regarding user perspectives on gender may prevent app developers and owners from identifying issues related to gender and proposing solutions for improvement. Users express many different opinions about apps in their reviews, from sharing their experiences, and reporting bugs, to requesting new features. In this study, we aim at unpacking gender discussions about apps from the user perspective by analysing app reviews. We first develop and evaluate several Machine Learning (ML) and Deep Learning (DL) classifiers that automatically detect gender reviews (i.e., reviews that contain discussions about gender). We apply our ML and DL classifiers on a manually constructed dataset of 1,440 app reviews from the Google App Store, composing 620 gender reviews and 820 non-gender reviews. Our best classifier achieves an F1-score of 90.77%. Second, our qualitative analysis of a randomly selected 388 out of 620 gender reviews shows that gender discussions in app reviews revolve around six topics: App Features, Appearance, Content, Company Policy and Censorship, Advertisement, and Community. Finally, we provide some practical implications and recommendations for developing gender-inclusive apps.",
        "authors": [
            {
                "name": "Mojtaba Shahin",
                "institution": "RMIT University",
                "country": "Australia"
            },
            {
                "name": "Mansooreh Zahedi",
                "institution": "The Univeristy of Melbourne",
                "country": "Australia"
            },
            {
                "name": "Hourieh Khalajzadeh",
                "institution": "Deakin University",
                "country": "Australia"
            },
            {
                "name": "Ali Rezaei Nasab",
                "institution": "Shiraz University",
                "country": null
            }
        ]
    },
    {
        "title": "The ABLoTS Approach for Bug Localization: is it replicable and generalizable?",
        "topics": "Technical Papers",
        "abstract": "Bug localization is the task of recommending source code locations (typically files) that probably contain the cause of a bug and hence need to be changed to fix the bug. Along these lines, information retrieval-based bug localization (IRBL) approaches have been adopted, which identify the most bug-prone files from the source code space. In current practice, a series of state-of-the-art IRBL techniques leverage the combination of different components, e.g., similar reports, version history, code structure, to achieve better performance. ABLoTS is a recently proposed approach with the core component, TraceScore that utilizes requirements and traceability information between different issue reports (i.e., feature requests and bug reports) to identify buggy source code snippets with promising results. To evaluate the accuracy of these results and obtain additional insights into the practical applicability of ABLoTS, in supporting of future more efficient and rapid replication and comparison, we conducted a replication study of this approach with the original data set and also on an extended data set. The extended data set includes 16 more projects comprising 25,893 bug reports and corresponding source code commits. While we find that the TraceScore component as the core of ABLoTS produces comparable results with the extended data set, we also find that the ABLoTS approach no longer achieves promising results due to an overlooked side-effect of incorrectly choosing a cut-off date that led to training data leaking into test data with significant effects on performance.",
        "authors": [
            {
                "name": "Feifei Niu",
                "institution": "University of Ottawa",
                "country": "Canada"
            },
            {
                "name": "Christoph Mayr-Dorn",
                "institution": "JOHANNES KEPLER UNIVERSITY LINZ",
                "country": null
            },
            {
                "name": "Wesley Assun\u00e7\u00e3o",
                "institution": "Johannes Kepler University Linz, Austria & Pontifical Catholic University of Rio de Janeiro, Brazil",
                "country": "Austria"
            },
            {
                "name": "Liguo Huang",
                "institution": "Southern Methodist University",
                "country": "United States"
            },
            {
                "name": "Jidong Ge",
                "institution": "Nanjing University",
                "country": "China"
            },
            {
                "name": "Bin Luo",
                "institution": "Nanjing University",
                "country": "China"
            },
            {
                "name": "Alexander Egyed",
                "institution": "Johannes Kepler University Linz",
                "country": "Austria"
            }
        ]
    },
    {
        "title": "She Elicits Requirements and He Tests: Software Engineering Gender Bias in Large Language Models",
        "topics": "Technical Papers",
        "abstract": "Implicit gender bias in software development is a well-documented issue, such as the association of technical roles with men. To address this bias, it is important to understand it in more detail. This study uses data mining techniques to investigate the extent to which 56 tasks related to software development, such as assigning GitHub issues and testing, are affected by implicit gender bias embedded in large language models. We systematically translated each task from English into a genderless language and back, and investigated the pronouns associated with each task. Based on translating each task 100 times in different permutations, we identify a significant disparity in the gendered pronoun associations with different tasks. Specifically, when starting with the pronoun \u201cshe\u201d, requirements elicitation was associated with the pronoun \u201che\u201d in only 6% of cases, while testing was associated with \u201che\u201d in 100% of cases. Additionally, tasks related to helping others had a 91% association with \u201che\u201d while the same association for tasks related to asking coworkers was only 52%. These findings reveal a clear pattern of gender bias related to software development tasks and have important implications for addressing this issue both in the training of large language models and in broader society.",
        "authors": [
            {
                "name": "Christoph Treude",
                "institution": "University of Melbourne",
                "country": "Australia"
            },
            {
                "name": "Hideaki Hata",
                "institution": "Shinshu University",
                "country": "Japan"
            }
        ]
    },
    {
        "title": "LLMSecEval: A Dataset of Natural Language Prompts for Security Evaluations",
        "topics": "Data and Tool Showcase Track",
        "abstract": "Large Language Models (LLMs) like Codex are powerful tools for performing code completion and code generation tasks as they are trained on billions of lines of code from publicly available sources. Moreover, these models are capable of generating code snippets from Natural Language (NL) descriptions by learning languages and programming practices from public GitHub repositories. Although LLMs promise an effortless NL-driven deployment of software applications, the security of the code they generate has not been extensively investigated nor documented. In this work, we present \\textit{LLMSecEval}, a dataset containing 150 NL prompts that can be leveraged for assessing the security performance of such models. Such prompts are NL descriptions of code snippets prone to various security vulnerabilities listed in MITRE\u2019s top 25 Common Weakness Enumeration (CWE) ranking. Each prompt in our dataset comes with a secure implementation example to facilitate comparative evaluations against code produced by LLMs. As a practical application, we show how \\textit{LLMSecEval} can be used for evaluating the security of snippets automatically generated from NL descriptions.",
        "authors": [
            {
                "name": "Catherine Tony",
                "institution": "Hamburg University of Technology",
                "country": "Germany"
            },
            {
                "name": "Markus Mutas",
                "institution": "Hamburg University of Technology",
                "country": "Germany"
            },
            {
                "name": "Nicol\u00e1s E. D\u00edaz Ferreyra",
                "institution": "Hamburg University of Technology",
                "country": "Germany"
            },
            {
                "name": "Riccardo Scandariato",
                "institution": "Hamburg University of Technology",
                "country": "Germany"
            }
        ]
    },
    {
        "title": "Defectors: A Large, Diverse Python Dataset for Defect Prediction",
        "topics": "Data and Tool Showcase Track",
        "abstract": "Defect prediction has been a popular research topic where deep learning has found numerous applications. However, these deep learning-based defect prediction models are often limited by the quality and size of their datasets. In this paper, we present Defectors, a large dataset for just-in-time and line-level defect prediction. Defectors consists of $\\approx$ 213K source code files ($\\approx$ 93K defective and $\\approx$ 120K defect-free) that span across 24 popular Python projects. These projects come from 18 different domains, including machine learning, automation, and internet-of-things. Such a scale and diversity make Defectors a suitable dataset for training deep learning models, especially transformer models that require large and diverse datasets. We also foresee several application areas of our dataset including defect prediction, defect explanation, and automated program repair.",
        "authors": [
            {
                "name": "Parvez Mahbub",
                "institution": "Dalhousie University",
                "country": "Canada"
            },
            {
                "name": "Ohiduzzaman Shuvo",
                "institution": "Dalhousie University",
                "country": null
            },
            {
                "name": "Masud Rahman",
                "institution": "Dalhousie University",
                "country": "Canada"
            }
        ]
    },
    {
        "title": "Tell Me Who Are You Talking to and I Will Tell You What Issues Need Your Skills",
        "topics": "Technical Papers",
        "abstract": "Abstract\u2014 Selecting an appropriate task is a challenging step for newcomers to Open Source Software (OSS) projects. To facilitate task selection, researchers and OSS projects have leveraged machine learning techniques, historical information, and textual analysis to label tasks (a.k.a. issues) with information such as the issue type and domain. These approaches are still far from mainstream adoption, possibly because of a lack of good predictors. Inspired by previous research, we advocate that label prediction might benefit from leveraging metrics derived from communication data and social network analysis (SNA) for issues in which social interaction occurs. Thus, we study how these \u201csocial metrics\u201d can improve the automatic labeling of open issues with API domains\u2014categories of APIs used in the source code that solves the issue\u2014which the literature shows that newcomers to the project consider relevant for task selection. We mined data from OSS projects\u2019 repositories and organized it in periods to reflect the seasonality of the contributors\u2019 project participation. We replicated metrics from previous work and added social metrics to the corpus to predict API-domain labels. Social metrics improved the performance of the classifiers compared to using only the issue description text in terms of precision, recall, and f-measure. Precision (0.945) increased by 18.7% and F-measure (0.963) by 17.7% for a project with high social activity. These results indicate that social metrics can help capture the patterns of social interactions in a software project and improve the labeling of issues in an issue tracker",
        "authors": [
            {
                "name": "Fabio Marcos De Abreu Santos",
                "institution": "Northern Arizona University",
                "country": "USA"
            },
            {
                "name": "Jacob Penney",
                "institution": "Northern Arizona University",
                "country": null
            },
            {
                "name": "Jo\u00e3o Felipe Pimentel",
                "institution": "Northern Arizona University",
                "country": null
            },
            {
                "name": "Igor Wiese",
                "institution": "Federal University of Technology",
                "country": null
            },
            {
                "name": "Igor Steinmacher",
                "institution": "Northern Arizona University",
                "country": null
            },
            {
                "name": "Marco Gerosa",
                "institution": "Northern Arizona University",
                "country": null
            }
        ]
    },
    {
        "title": "GitHub OSS Governance File Dataset",
        "topics": "Data and Tool Showcase Track",
        "abstract": "Open-source Software (OSS) has become a valuable resource in both industry and academia over the last few decades. Despite the innovative structures they develop to support the projects, OSS projects and their communities have complex needs and face risks such as getting abandoned. To manage the internal social dynamics and community evolution, OSS developer communities have started relying on written governance documents that assign roles and responsibilities to different community actors.",
        "authors": [
            {
                "name": "Yibo Yan",
                "institution": "University of California, Davis",
                "country": "United States"
            },
            {
                "name": "Seth Frey",
                "institution": "University of California, Davis",
                "country": "United States"
            },
            {
                "name": "Amy Zhang",
                "institution": "University of Washington, Seattle",
                "country": "United States"
            },
            {
                "name": "Vladimir Filkov",
                "institution": "University of California at Davis, USA",
                "country": "United States"
            },
            {
                "name": "Likang Yin",
                "institution": "University of California at Davis",
                "country": "United States"
            }
        ]
    }
]